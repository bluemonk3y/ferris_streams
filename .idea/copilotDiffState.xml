<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.github/workflows/rust.yml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.github/workflows/rust.yml" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/sql/join_tests.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/sql/join_tests.rs" />
              <option name="originalContent" value="/*!&#10;# Tests for JOIN Operations&#10;&#10;Comprehensive test suite for all JOIN types (INNER, LEFT, RIGHT, FULL OUTER) and windowed JOINs in streaming SQL.&#10;*/&#10;&#10;use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use std::collections::HashMap;&#10;use tokio::sync::mpsc;&#10;use ferrisstreams::ferris::serialization::JsonFormat;&#10;&#10;fn create_test_record_for_join() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(100.0));&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;async fn execute_join_query(&#10;    query: &amp;str,&#10;) -&gt; Result&lt;Vec&lt;HashMap&lt;String, serde_json::Value&gt;&gt;, Box&lt;dyn std::error::Error&gt;&gt; {&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let serialization_format = std::sync::Arc::new(JsonFormat);&#10;    let mut engine = StreamExecutionEngine::new(tx, serialization_format.clone());&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    let parsed_query = parser.parse(query)?;&#10;    let record = create_test_record_with_join_fields();&#10;&#10;    // Convert StreamRecord to HashMap&lt;String, serde_json::Value&gt;&#10;    let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;        .fields&#10;        .into_iter()&#10;        .map(|(k, v)| {&#10;            let json_val = match v {&#10;                FieldValue::Integer(i) =&gt; serde_json::Value::Number(serde_json::Number::from(i)),&#10;                FieldValue::Float(f) =&gt; {&#10;                    serde_json::Value::Number(serde_json::Number::from_f64(f).unwrap_or(0.into()))&#10;                }&#10;                FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                FieldValue::Null =&gt; serde_json::Value::Null,&#10;                _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;            };&#10;            (k, json_val)&#10;        })&#10;        .collect();&#10;&#10;    engine.execute(&amp;parsed_query, json_record).await?;&#10;&#10;    let mut results = Vec::new();&#10;    while let Ok(result) = rx.try_recv() {&#10;        results.push(result);&#10;    }&#10;    Ok(results)&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_basic_inner_join() {&#10;    // Test basic INNER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    // This should work with our mock implementation&#10;    let result = execute_join_query(query).await;&#10;&#10;    // For now, expect an error since we haven't implemented the parser yet&#10;    // Once the parser supports JOIN, this should succeed&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_alias() {&#10;    // Test JOIN with table aliases&#10;    let query = &quot;SELECT l.name, r.right_name FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_where_clause() {&#10;    // Test JOIN combined with WHERE clause&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id WHERE l.amount &gt; 50&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_field_access() {&#10;    // Test accessing joined fields&#10;    let query = &quot;SELECT id, name, right_name, right_value FROM left_stream INNER JOIN right_stream ON id = right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_multiple_joins() {&#10;    // Test multiple JOIN clauses (will be supported when parser is extended)&#10;    let query = &quot;SELECT * FROM stream1 s1 INNER JOIN stream2 s2 ON s1.id = s2.id INNER JOIN stream3 s3 ON s2.id = s3.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;    // Now supports multiple JOINs&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_outer_join() {&#10;    // Test LEFT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream LEFT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    // Should succeed now that parser supports LEFT JOIN&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_join_short_syntax() {&#10;    // Test LEFT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream LEFT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_outer_join() {&#10;    // Test RIGHT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream RIGHT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_join_short_syntax() {&#10;    // Test RIGHT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream RIGHT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_full_outer_join() {&#10;    // Test FULL OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream FULL OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join() {&#10;    // Test JOIN with WITHIN clause for temporal joins&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id WITHIN INTERVAL '5' MINUTES&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_seconds() {&#10;    // Test JOIN with WITHIN clause using seconds&#10;    let query = &quot;SELECT * FROM orders INNER JOIN payments p ON orders.id = p.order_id WITHIN INTERVAL '30' SECONDS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_hours() {&#10;    // Test JOIN with WITHIN clause using hours&#10;    let query = &quot;SELECT * FROM sessions LEFT JOIN events e ON sessions.user_id = e.user_id WITHIN INTERVAL '2' HOURS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_complex_condition() {&#10;    // Test JOIN with complex ON condition&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id AND l.amount &gt; 100&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_specific_fields() {&#10;    // Test JOIN with specific field selection - simplified to avoid alias issues for now&#10;    let query =&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream r ON left_stream.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_parsing_validation() {&#10;    // Test that invalid JOIN syntax is properly rejected&#10;    let invalid_queries = vec![&#10;        &quot;SELECT * FROM left_stream JOIN&quot;,              // Missing right side&#10;        &quot;SELECT * FROM left_stream JOIN right_stream&quot;, // Missing ON clause&#10;        &quot;SELECT * FROM left_stream INNER&quot;,             // Incomplete JOIN&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream ON&quot;, // Missing condition&#10;        &quot;SELECT * FROM left_stream FULL JOIN right_stream ON l.id = r.id&quot;, // FULL without OUTER&#10;    ];&#10;&#10;    for query in invalid_queries {&#10;        let result = execute_join_query(query).await;&#10;        assert!(result.is_err(), &quot;Query should have failed: {}&quot;, query);&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_stream_table_join_syntax() {&#10;    // Test stream-table JOIN which should be optimized differently&#10;    let query = &quot;SELECT s.user_id, s.event_type, t.user_name FROM events s INNER JOIN user_table t ON s.user_id = t.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;fn create_test_record_with_join_fields() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;user_id&quot;.to_string(), FieldValue::Integer(100));&#10;    fields.insert(&quot;order_id&quot;.to_string(), FieldValue::Integer(500));&#10;    fields.insert(&#10;        &quot;name&quot;.to_string(),&#10;        FieldValue::String(&quot;Test User&quot;.to_string()),&#10;    );&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(250.0));&#10;    fields.insert(&#10;        &quot;event_type&quot;.to_string(),&#10;        FieldValue::String(&quot;click&quot;.to_string()),&#10;    );&#10;    fields.insert(&#10;        &quot;status&quot;.to_string(),&#10;        FieldValue::String(&quot;active&quot;.to_string()),&#10;    );&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_execution_logic() {&#10;    // Test that the JOIN execution logic actually works with the parser&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let mut engine = StreamExecutionEngine::new(tx);&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    // This should parse successfully and execute the JOIN logic&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    match parser.parse(query) {&#10;        Ok(parsed_query) =&gt; {&#10;            let record = create_test_record_with_join_fields();&#10;&#10;            // Convert to JSON format for execution&#10;            let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;                .fields&#10;                .into_iter()&#10;                .map(|(k, v)| {&#10;                    let json_val = match v {&#10;                        FieldValue::Integer(i) =&gt; {&#10;                            serde_json::Value::Number(serde_json::Number::from(i))&#10;                        }&#10;                        FieldValue::Float(f) =&gt; serde_json::Value::Number(&#10;                            serde_json::Number::from_f64(f).unwrap_or(0.into()),&#10;                        ),&#10;                        FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                        FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                        FieldValue::Null =&gt; serde_json::Value::Null,&#10;                        _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;                    };&#10;                    (k, json_val)&#10;                })&#10;                .collect();&#10;&#10;            // Execute the query - this tests the JOIN execution engine&#10;            let execution_result = engine.execute(&amp;parsed_query, json_record).await;&#10;&#10;            // Should either succeed or fail gracefully with proper error&#10;            assert!(&#10;                execution_result.is_ok()&#10;                    || execution_result.unwrap_err().to_string().contains(&quot;JOIN&quot;)&#10;            );&#10;        }&#10;        Err(e) =&gt; {&#10;            // Parser should succeed with the new JOIN parsing logic&#10;            panic!(&quot;Failed to parse JOIN query: {}&quot;, e);&#10;        }&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="/*!&#10;# Tests for JOIN Operations&#10;&#10;Comprehensive test suite for all JOIN types (INNER, LEFT, RIGHT, FULL OUTER) and windowed JOINs in streaming SQL.&#10;*/&#10;&#10;use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use std::collections::HashMap;&#10;use tokio::sync::mpsc;&#10;use ferrisstreams::ferris::serialization::JsonFormat;&#10;&#10;fn create_test_record_for_join() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(100.0));&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;async fn execute_join_query(&#10;    query: &amp;str,&#10;) -&gt; Result&lt;Vec&lt;HashMap&lt;String, serde_json::Value&gt;&gt;, Box&lt;dyn std::error::Error&gt;&gt; {&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let serialization_format = std::sync::Arc::new(JsonFormat);&#10;    let mut engine = StreamExecutionEngine::new(tx, serialization_format.clone());&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    let parsed_query = parser.parse(query)?;&#10;    let record = create_test_record_with_join_fields();&#10;&#10;    // Convert StreamRecord to HashMap&lt;String, serde_json::Value&gt;&#10;    let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;        .fields&#10;        .into_iter()&#10;        .map(|(k, v)| {&#10;            let json_val = match v {&#10;                FieldValue::Integer(i) =&gt; serde_json::Value::Number(serde_json::Number::from(i)),&#10;                FieldValue::Float(f) =&gt; {&#10;                    serde_json::Value::Number(serde_json::Number::from_f64(f).unwrap_or(0.into()))&#10;                }&#10;                FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                FieldValue::Null =&gt; serde_json::Value::Null,&#10;                _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;            };&#10;            (k, json_val)&#10;        })&#10;        .collect();&#10;&#10;    engine.execute(&amp;parsed_query, json_record).await?;&#10;&#10;    let mut results = Vec::new();&#10;    while let Ok(result) = rx.try_recv() {&#10;        results.push(result);&#10;    }&#10;    Ok(results)&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_basic_inner_join() {&#10;    // Test basic INNER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    // This should work with our mock implementation&#10;    let result = execute_join_query(query).await;&#10;&#10;    // For now, expect an error since we haven't implemented the parser yet&#10;    // Once the parser supports JOIN, this should succeed&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_alias() {&#10;    // Test JOIN with table aliases&#10;    let query = &quot;SELECT l.name, r.right_name FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_where_clause() {&#10;    // Test JOIN combined with WHERE clause&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id WHERE l.amount &gt; 50&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_field_access() {&#10;    // Test accessing joined fields&#10;    let query = &quot;SELECT id, name, right_name, right_value FROM left_stream INNER JOIN right_stream ON id = right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_multiple_joins() {&#10;    // Test multiple JOIN clauses (will be supported when parser is extended)&#10;    let query = &quot;SELECT * FROM stream1 s1 INNER JOIN stream2 s2 ON s1.id = s2.id INNER JOIN stream3 s3 ON s2.id = s3.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;    // Now supports multiple JOINs&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_outer_join() {&#10;    // Test LEFT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream LEFT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    // Should succeed now that parser supports LEFT JOIN&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_join_short_syntax() {&#10;    // Test LEFT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream LEFT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_outer_join() {&#10;    // Test RIGHT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream RIGHT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_join_short_syntax() {&#10;    // Test RIGHT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream RIGHT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_full_outer_join() {&#10;    // Test FULL OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream FULL OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join() {&#10;    // Test JOIN with WITHIN clause for temporal joins&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id WITHIN INTERVAL '5' MINUTES&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_seconds() {&#10;    // Test JOIN with WITHIN clause using seconds&#10;    let query = &quot;SELECT * FROM orders INNER JOIN payments p ON orders.id = p.order_id WITHIN INTERVAL '30' SECONDS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_hours() {&#10;    // Test JOIN with WITHIN clause using hours&#10;    let query = &quot;SELECT * FROM sessions LEFT JOIN events e ON sessions.user_id = e.user_id WITHIN INTERVAL '2' HOURS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_complex_condition() {&#10;    // Test JOIN with complex ON condition&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id AND l.amount &gt; 100&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_specific_fields() {&#10;    // Test JOIN with specific field selection - simplified to avoid alias issues for now&#10;    let query =&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream r ON left_stream.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_parsing_validation() {&#10;    // Test that invalid JOIN syntax is properly rejected&#10;    let invalid_queries = vec![&#10;        &quot;SELECT * FROM left_stream JOIN&quot;,              // Missing right side&#10;        &quot;SELECT * FROM left_stream JOIN right_stream&quot;, // Missing ON clause&#10;        &quot;SELECT * FROM left_stream INNER&quot;,             // Incomplete JOIN&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream ON&quot;, // Missing condition&#10;        &quot;SELECT * FROM left_stream FULL JOIN right_stream ON l.id = r.id&quot;, // FULL without OUTER&#10;    ];&#10;&#10;    for query in invalid_queries {&#10;        let result = execute_join_query(query).await;&#10;        assert!(result.is_err(), &quot;Query should have failed: {}&quot;, query);&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_stream_table_join_syntax() {&#10;    // Test stream-table JOIN which should be optimized differently&#10;    let query = &quot;SELECT s.user_id, s.event_type, t.user_name FROM events s INNER JOIN user_table t ON s.user_id = t.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;fn create_test_record_with_join_fields() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;user_id&quot;.to_string(), FieldValue::Integer(100));&#10;    fields.insert(&quot;order_id&quot;.to_string(), FieldValue::Integer(500));&#10;    fields.insert(&#10;        &quot;name&quot;.to_string(),&#10;        FieldValue::String(&quot;Test User&quot;.to_string()),&#10;    );&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(250.0));&#10;    fields.insert(&#10;        &quot;event_type&quot;.to_string(),&#10;        FieldValue::String(&quot;click&quot;.to_string()),&#10;    );&#10;    fields.insert(&#10;        &quot;status&quot;.to_string(),&#10;        FieldValue::String(&quot;active&quot;.to_string()),&#10;    );&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_execution_logic() {&#10;    // Test that the JOIN execution logic actually works with the parser&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let mut engine = StreamExecutionEngine::new(tx);&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    // This should parse successfully and execute the JOIN logic&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    match parser.parse(query) {&#10;        Ok(parsed_query) =&gt; {&#10;            let record = create_test_record_with_join_fields();&#10;&#10;            // Convert to JSON format for execution&#10;            let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;                .fields&#10;                .into_iter()&#10;                .map(|(k, v)| {&#10;                    let json_val = match v {&#10;                        FieldValue::Integer(i) =&gt; {&#10;                            serde_json::Value::Number(serde_json::Number::from(i))&#10;                        }&#10;                        FieldValue::Float(f) =&gt; serde_json::Value::Number(&#10;                            serde_json::Number::from_f64(f).unwrap_or(0.into()),&#10;                        ),&#10;                        FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                        FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                        FieldValue::Null =&gt; serde_json::Value::Null,&#10;                        _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;                    };&#10;                    (k, json_val)&#10;                })&#10;                .collect();&#10;&#10;            // Execute the query - this tests the JOIN execution engine&#10;            let execution_result = engine.execute(&amp;parsed_query, json_record).await;&#10;&#10;            // Should either succeed or fail gracefully with proper error&#10;            assert!(&#10;                execution_result.is_ok()&#10;                    || execution_result.unwrap_err().to_string().contains(&quot;JOIN&quot;)&#10;            );&#10;        }&#10;        Err(e) =&gt; {&#10;            // Parser should succeed with the new JOIN parsing logic&#10;            panic!(&quot;Failed to parse JOIN query: {}&quot;, e);&#10;        }&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/sql/limit_tests.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/sql/limit_tests.rs" />
              <option name="originalContent" value="use ferrisstreams::ferris::sql::execution::StreamExecutionEngine;&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use ferrisstreams::ferris::serialization::{JsonFormat, InternalValue};&#10;use std::collections::HashMap;&#10;use std::sync::Arc;&#10;use tokio::sync::mpsc;&#10;&#10;#[cfg(test)]&#10;mod tests {&#10;    use super::*;&#10;&#10;    #[test]&#10;    fn test_limit_parsing() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test parsing queries with LIMIT&#10;        let queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT 10&quot;,&#10;            &quot;SELECT customer_id, amount FROM orders LIMIT 5&quot;,&#10;            &quot;SELECT * FROM orders WHERE amount &gt; 100 LIMIT 3&quot;,&#10;        ];&#10;&#10;        for query in queries {&#10;            let result = parser.parse(query);&#10;            assert!(result.is_ok(), &quot;Failed to parse query: {}&quot;, query);&#10;&#10;            // Verify limit is parsed correctly&#10;            if let Ok(parsed_query) = result {&#10;                match parsed_query {&#10;                    ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                        assert!(&#10;                            limit.is_some(),&#10;                            &quot;LIMIT should be parsed for query: {}&quot;,&#10;                            query&#10;                        );&#10;                    }&#10;                    _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;                }&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_values() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        let test_cases = vec![&#10;            (&quot;SELECT * FROM orders LIMIT 1&quot;, 1),&#10;            (&quot;SELECT * FROM orders LIMIT 10&quot;, 10),&#10;            (&quot;SELECT * FROM orders LIMIT 100&quot;, 100),&#10;            (&quot;SELECT * FROM orders LIMIT 1000&quot;, 1000),&#10;        ];&#10;&#10;        for (query, expected_limit) in test_cases {&#10;            let result = parser.parse(query).unwrap();&#10;            match result {&#10;                ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                    assert_eq!(&#10;                        limit,&#10;                        Some(expected_limit),&#10;                        &quot;Limit value mismatch for query: {}&quot;,&#10;                        query&#10;                    );&#10;                }&#10;                _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;            }&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_execution_basic() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 2&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders LIMIT 2&quot;)&#10;            .unwrap();&#10;&#10;        // Create test records&#10;        let mut records: Vec&lt;HashMap&lt;String, InternalValue&gt;&gt; = Vec::new();&#10;        for i in 1..=3 {&#10;            let mut record = HashMap::new();&#10;            record.insert(&#10;                &quot;customer_id&quot;.to_string(),&#10;                InternalValue::Integer(i),&#10;            );&#10;            record.insert(&#10;                &quot;amount&quot;.to_string(),&#10;                InternalValue::Number(100.0 * i as f64),&#10;            );&#10;            // Execute each record individually&#10;            engine.execute(&amp;query, record).await.unwrap();&#10;        }&#10;&#10;        // Check results&#10;        let mut count = 0;&#10;        while let Some(_result) = rx.recv().await {&#10;            count += 1;&#10;            assert!(count &lt;= 2, &quot;Should not receive more than 2 records due to LIMIT&quot;);&#10;        }&#10;        assert_eq!(count, 2, &quot;Should receive exactly 2 records due to LIMIT&quot;);&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_where_clause() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with WHERE and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders WHERE amount &gt; 150 LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(3));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(300.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output (first record matching WHERE clause)&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;amount&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::Number(amount)) =&gt; {&#10;                    assert_eq!(*id, 2);&#10;                    assert_eq!(*amount, 200.0);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second matching record due to LIMIT&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_zero() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 0&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id FROM orders LIMIT 0&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&#10;            &quot;customer_id&quot;.to_string(),&#10;            Value::Number(serde_json::Number::from(1)),&#10;        );&#10;&#10;        // Execute record&#10;        let result = engine.execute(&amp;query, record).await;&#10;        assert!(result.is_ok());&#10;&#10;        // Should not receive any output due to LIMIT 0&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive any records due to LIMIT 0&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_csas() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse CSAS query with LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;CREATE STREAM limited_orders AS SELECT customer_id, amount FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;        } else {&#10;            panic!(&quot;Expected to receive one record&quot;);&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT in CSAS&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_system_columns() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with system columns and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, _timestamp, _partition FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;            assert!(output.contains_key(&quot;_timestamp&quot;));&#10;            assert!(output.contains_key(&quot;_partition&quot;));&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_headers() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Create test headers&#10;        let mut headers = HashMap::new();&#10;        headers.insert(&quot;source&quot;.to_string(), &quot;test-app&quot;.to_string());&#10;&#10;        // Parse query with header function and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, HEADER('source') AS source FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute_with_headers(&amp;query, record, headers.clone()).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute_with_headers(&amp;query, record, headers).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;source&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::String(source)) =&gt; {&#10;                    assert_eq!(*id, 1);&#10;                    assert_eq!(source, &quot;test-app&quot;);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_errors() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test invalid LIMIT values&#10;        let invalid_queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT -1&quot;,  // Negative not handled gracefully&#10;            &quot;SELECT * FROM orders LIMIT abc&quot;, // Non-numeric&#10;            &quot;SELECT * FROM orders LIMIT 1.5&quot;, // Float&#10;        ];&#10;&#10;        for query in invalid_queries {&#10;            let result = parser.parse(query);&#10;            // These should fail at parse time or be handled gracefully&#10;            if result.is_ok() {&#10;                println!(&quot;Query parsed unexpectedly: {}&quot;, query);&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_query_without_limit() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test query without LIMIT clause&#10;        let query = &quot;SELECT * FROM orders WHERE amount &gt; 100&quot;;&#10;        let result = parser.parse(query).unwrap();&#10;&#10;        match result {&#10;            ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                assert_eq!(limit, None, &quot;Query without LIMIT should have None limit&quot;);&#10;            }&#10;            _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;        }&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="use ferrisstreams::ferris::sql::execution::StreamExecutionEngine;&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use ferrisstreams::ferris::serialization::{JsonFormat, InternalValue};&#10;use std::collections::HashMap;&#10;use std::sync::Arc;&#10;use tokio::sync::mpsc;&#10;&#10;#[cfg(test)]&#10;mod tests {&#10;    use super::*;&#10;&#10;    #[test]&#10;    fn test_limit_parsing() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test parsing queries with LIMIT&#10;        let queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT 10&quot;,&#10;            &quot;SELECT customer_id, amount FROM orders LIMIT 5&quot;,&#10;            &quot;SELECT * FROM orders WHERE amount &gt; 100 LIMIT 3&quot;,&#10;        ];&#10;&#10;        for query in queries {&#10;            let result = parser.parse(query);&#10;            assert!(result.is_ok(), &quot;Failed to parse query: {}&quot;, query);&#10;&#10;            // Verify limit is parsed correctly&#10;            if let Ok(parsed_query) = result {&#10;                match parsed_query {&#10;                    ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                        assert!(&#10;                            limit.is_some(),&#10;                            &quot;LIMIT should be parsed for query: {}&quot;,&#10;                            query&#10;                        );&#10;                    }&#10;                    _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;                }&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_values() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        let test_cases = vec![&#10;            (&quot;SELECT * FROM orders LIMIT 1&quot;, 1),&#10;            (&quot;SELECT * FROM orders LIMIT 10&quot;, 10),&#10;            (&quot;SELECT * FROM orders LIMIT 100&quot;, 100),&#10;            (&quot;SELECT * FROM orders LIMIT 1000&quot;, 1000),&#10;        ];&#10;&#10;        for (query, expected_limit) in test_cases {&#10;            let result = parser.parse(query).unwrap();&#10;            match result {&#10;                ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                    assert_eq!(&#10;                        limit,&#10;                        Some(expected_limit),&#10;                        &quot;Limit value mismatch for query: {}&quot;,&#10;                        query&#10;                    );&#10;                }&#10;                _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;            }&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_execution_basic() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 2&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders LIMIT 2&quot;)&#10;            .unwrap();&#10;&#10;        // Create test records&#10;        let mut records: Vec&lt;HashMap&lt;String, InternalValue&gt;&gt; = Vec::new();&#10;        for i in 1..=3 {&#10;            let mut record = HashMap::new();&#10;            record.insert(&#10;                &quot;customer_id&quot;.to_string(),&#10;                InternalValue::Integer(i),&#10;            );&#10;            record.insert(&#10;                &quot;amount&quot;.to_string(),&#10;                InternalValue::Number(100.0 * i as f64),&#10;            );&#10;            // Execute each record individually&#10;            engine.execute(&amp;query, record).await.unwrap();&#10;        }&#10;&#10;        // Check results&#10;        let mut count = 0;&#10;        while let Some(_result) = rx.recv().await {&#10;            count += 1;&#10;            assert!(count &lt;= 2, &quot;Should not receive more than 2 records due to LIMIT&quot;);&#10;        }&#10;        assert_eq!(count, 2, &quot;Should receive exactly 2 records due to LIMIT&quot;);&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_where_clause() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with WHERE and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders WHERE amount &gt; 150 LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(3));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(300.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output (first record matching WHERE clause)&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;amount&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::Number(amount)) =&gt; {&#10;                    assert_eq!(*id, 2);&#10;                    assert_eq!(*amount, 200.0);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second matching record due to LIMIT&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_zero() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 0&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id FROM orders LIMIT 0&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should not receive any output due to LIMIT 0&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive any records due to LIMIT 0&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_csas() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse CSAS query with LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;CREATE STREAM limited_orders AS SELECT customer_id, amount FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;        } else {&#10;            panic!(&quot;Expected to receive one record&quot;);&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT in CSAS&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_system_columns() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with system columns and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, _timestamp, _partition FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;            assert!(output.contains_key(&quot;_timestamp&quot;));&#10;            assert!(output.contains_key(&quot;_partition&quot;));&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_headers() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Create test headers&#10;        let mut headers = HashMap::new();&#10;        headers.insert(&quot;source&quot;.to_string(), &quot;test-app&quot;.to_string());&#10;&#10;        // Parse query with header function and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, HEADER('source') AS source FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute_with_headers(&amp;query, record, headers.clone()).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute_with_headers(&amp;query, record, headers).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;source&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::String(source)) =&gt; {&#10;                    assert_eq!(*id, 1);&#10;                    assert_eq!(source, &quot;test-app&quot;);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_errors() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test invalid LIMIT values&#10;        let invalid_queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT -1&quot;,  // Negative not handled gracefully&#10;            &quot;SELECT * FROM orders LIMIT abc&quot;, // Non-numeric&#10;            &quot;SELECT * FROM orders LIMIT 1.5&quot;, // Float&#10;        ];&#10;&#10;        for query in invalid_queries {&#10;            let result = parser.parse(query);&#10;            // These should fail at parse time or be handled gracefully&#10;            if result.is_ok() {&#10;                println!(&quot;Query parsed unexpectedly: {}&quot;, query);&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_query_without_limit() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test query without LIMIT clause&#10;        let query = &quot;SELECT * FROM orders WHERE amount &gt; 100&quot;;&#10;        let result = parser.parse(query).unwrap();&#10;&#10;        match result {&#10;            ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                assert_eq!(limit, None, &quot;Query without LIMIT should have None limit&quot;);&#10;            }&#10;            _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;        }&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/unit/sql/execution_test.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/unit/sql/execution_test.rs" />
              <option name="originalContent" value="#[cfg(test)]&#10;mod tests {&#10;    use ferrisstreams::ferris::sql::ast::{BinaryOperator, Expr, LiteralValue};&#10;    use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;    use ferrisstreams::ferris::serialization::{InternalValue, JsonFormat};&#10;    use std::collections::HashMap;&#10;    use std::sync::Arc;&#10;    use tokio::sync::mpsc;&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        let record = StreamRecord {&#10;            fields: record.into_iter()&#10;                .map(|(k, v)| (k, FieldValue::String(v)))&#10;                .collect(),&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Test cases for LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, true),      // Matches prefix&#10;            (&quot;%World&quot;, true),      // Matches suffix&#10;            (&quot;%llo%&quot;, true),       // Matches substring&#10;            (&quot;Hello_World&quot;, true), // Matches exact with underscore&#10;            (&quot;hello%&quot;, false),     // Case sensitive no match&#10;            (&quot;Goodbye%&quot;, false),   // No match&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::Like,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed&quot;);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_not_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        let record = StreamRecord {&#10;            fields: record.into_iter()&#10;                .map(|(k, v)| (k, FieldValue::String(v)))&#10;                .collect(),&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Test cases for NOT LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, false),     // Matches prefix (so NOT LIKE is false)&#10;            (&quot;%World&quot;, false),     // Matches suffix (so NOT LIKE is false)&#10;            (&quot;%llo%&quot;, false),      // Matches substring (so NOT LIKE is false)&#10;            (&quot;Hello_World&quot;, false),// Matches exact with underscore (so NOT LIKE is false)&#10;            (&quot;hello%&quot;, true),      // Case sensitive no match (so NOT LIKE is true)&#10;            (&quot;Goodbye%&quot;, true),    // No match (so NOT LIKE is true)&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::NotLike,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;NOT LIKE operator evaluation failed&quot;);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator_edge_cases() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record with various types&#10;        let mut fields = HashMap::new();&#10;        fields.insert(&quot;text_field&quot;.to_string(), FieldValue::String(&quot;Hello World&quot;.to_string()));&#10;        fields.insert(&quot;null_field&quot;.to_string(), FieldValue::Null);&#10;        fields.insert(&quot;number_field&quot;.to_string(), FieldValue::Integer(123));&#10;&#10;        let record = StreamRecord {&#10;            fields,&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Edge cases&#10;        let test_cases = vec![&#10;            // Empty pattern&#10;            (&quot;&quot;, true),&#10;            // Just wildcards&#10;            (&quot;%&quot;, true),&#10;            (&quot;%%&quot;, true),&#10;            (&quot;___&quot;, false),  // Three underscores won't match &quot;Hello World&quot;&#10;            // Escape characters (if implemented)&#10;            (&quot;Hello\\_World&quot;, true),&#10;            // Complex patterns&#10;            (&quot;H%W%d&quot;, true),&#10;            (&quot;%H%W%d%&quot;, true),&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::Like,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed for pattern: {}&quot;, pattern);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;&#10;        // Test NULL field&#10;        let expr = Expr::BinaryOp {&#10;            left: Box::new(Expr::Column(&quot;null_field&quot;.to_string())),&#10;            op: BinaryOperator::Like,&#10;            right: Box::new(Expr::Literal(LiteralValue::String(&quot;%&quot;.to_string()))),&#10;        };&#10;        let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;        assert!(result.is_ok());&#10;        assert_eq!(result.unwrap(), false);  // NULL LIKE anything should be false&#10;&#10;        // Test numeric field with LIKE&#10;        let expr = Expr::BinaryOp {&#10;            left: Box::new(Expr::Column(&quot;number_field&quot;.to_string())),&#10;            op: BinaryOperator::Like,&#10;            right: Box::new(Expr::Literal(LiteralValue::String(&quot;%3&quot;.to_string()))),&#10;        };&#10;        let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;        assert!(result.is_ok());&#10;        assert_eq!(result.unwrap(), true);  // &quot;123&quot; LIKE &quot;%3&quot; should be true&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="#[cfg(test)]&#10;mod tests {&#10;    use ferrisstreams::ferris::sql::ast::{BinaryOperator, Expr, LiteralValue, SelectField, StreamSource, StreamingQuery};&#10;    use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;    use ferrisstreams::ferris::serialization::JsonFormat;&#10;    use std::collections::HashMap;&#10;    use std::sync::Arc;&#10;    use tokio::sync::mpsc;&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator() {&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        &#10;        // Test cases for LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, true),      // Matches prefix&#10;            (&quot;%World&quot;, true),      // Matches suffix&#10;            (&quot;%llo%&quot;, true),       // Matches substring&#10;            (&quot;Hello_World&quot;, true), // Matches exact with underscore&#10;            (&quot;hello%&quot;, false),     // Case sensitive no match&#10;            (&quot;Goodbye%&quot;, false),   // No match&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::Like,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed&quot;);&#10;            let output = result.unwrap();&#10;            // The output will be true/false based on whether any results were produced&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_not_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;&#10;        // Test cases for NOT LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, false),     // Matches prefix (so NOT LIKE is false)&#10;            (&quot;%World&quot;, false),     // Matches suffix (so NOT LIKE is false)&#10;            (&quot;%llo%&quot;, false),      // Matches substring (so NOT LIKE is false)&#10;            (&quot;Hello_World&quot;, false),// Matches exact with underscore (so NOT LIKE is false)&#10;            (&quot;hello%&quot;, true),      // Case sensitive no match (so NOT LIKE is true)&#10;            (&quot;Goodbye%&quot;, true),    // No match (so NOT LIKE is true)&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::NotLike,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;NOT LIKE operator evaluation failed&quot;);&#10;            let output = result.unwrap();&#10;            // The output will be true/false based on whether any results were produced&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator_edge_cases() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record with various types&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        record.insert(&quot;null_field&quot;.to_string(), &quot;null&quot;.to_string());  // Represents NULL&#10;        record.insert(&quot;number_field&quot;.to_string(), &quot;123&quot;.to_string());  // Number as string&#10;&#10;        // Edge cases&#10;        let test_cases = vec![&#10;            // Empty pattern&#10;            (&quot;&quot;, true),&#10;            // Just wildcards&#10;            (&quot;%&quot;, true),&#10;            (&quot;%%&quot;, true),&#10;            (&quot;___&quot;, false),  // Three underscores won't match &quot;Hello World&quot;&#10;            // Escape characters (if implemented)&#10;            (&quot;Hello\\_World&quot;, true),&#10;            // Complex patterns&#10;            (&quot;H%W%d&quot;, true),&#10;            (&quot;%H%W%d%&quot;, true),&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::Like,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed for pattern: {}&quot;, pattern);&#10;            let output = result.unwrap();&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;&#10;        // Test NULL field&#10;        let query = StreamingQuery::Select {&#10;            fields: vec![SelectField::Expression {&#10;                expr: Expr::BinaryOp {&#10;                    left: Box::new(Expr::Column(&quot;null_field&quot;.to_string())),&#10;                    op: BinaryOperator::Like,&#10;                    right: Box::new(Expr::Literal(LiteralValue::String(&quot;%&quot;.to_string()))),&#10;                },&#10;                alias: None,&#10;            }],&#10;            from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;            where_clause: None,&#10;            joins: None,&#10;            group_by: None,&#10;            having: None,&#10;            window: None,&#10;            limit: None,&#10;        };&#10;&#10;        let result = engine.execute(&amp;query, record.clone()).await;&#10;        assert!(result.is_ok());&#10;        let output = result.unwrap();&#10;        assert_eq!(output.is_ok(), false);  // NULL LIKE anything should be false&#10;&#10;        // Test numeric field with LIKE&#10;        let query = StreamingQuery::Select {&#10;            fields: vec![SelectField::Expression {&#10;                expr: Expr::BinaryOp {&#10;                    left: Box::new(Expr::Column(&quot;number_field&quot;.to_string())),&#10;                    op: BinaryOperator::Like,&#10;                    right: Box::new(Expr::Literal(LiteralValue::String(&quot;%3&quot;.to_string()))),&#10;                },&#10;                alias: None,&#10;            }],&#10;            from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;            where_clause: None,&#10;            joins: None,&#10;            group_by: None,&#10;            having: None,&#10;            window: None,&#10;            limit: None,&#10;        };&#10;&#10;        let result = engine.execute(&amp;query, record.clone()).await;&#10;        assert!(result.is_ok());&#10;        let output = result.unwrap();&#10;        assert_eq!(output.is_ok(), true);  // &quot;123&quot; LIKE &quot;%3&quot; should be true&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>