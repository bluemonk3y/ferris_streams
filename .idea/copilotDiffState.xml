<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.github/workflows/rust.yml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.github/workflows/rust.yml" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/ferris/serialization/mod.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/ferris/serialization/mod.rs" />
              <option name="originalContent" value="//! Pluggable serialization interface for FerrisStreams&#10;//!&#10;//! This module provides a comprehensive trait-based approach to serialization that allows&#10;//! the system to work with multiple data formats (JSON, Avro, Protobuf, etc.) instead&#10;//! of being hardcoded to JSON.&#10;//!&#10;//! # Features&#10;//!&#10;//! - **JSON**: Always available, human-readable, good for development&#10;//! - **Avro**: Feature-gated (`avro`), schema-based, supports evolution&#10;//! - **Protobuf**: Feature-gated (`protobuf`), very compact, high performance&#10;//!&#10;//! # Quick Start&#10;//!&#10;//! ```rust&#10;//! use ferrisstreams::ferris::serialization::{SerializationFormatFactory, FieldValue};&#10;//! use std::collections::HashMap;&#10;//!&#10;//! # fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {&#10;//! // Create a format&#10;//! let format = SerializationFormatFactory::create_format(&quot;json&quot;)?;&#10;//!&#10;//! // Create a record&#10;//! let mut record = HashMap::new();&#10;//! record.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;//! record.insert(&quot;age&quot;.to_string(), FieldValue::Integer(30));&#10;//!&#10;//! // Serialize and deserialize&#10;//! let bytes = format.serialize_record(&amp;record)?;&#10;//! let restored = format.deserialize_record(&amp;bytes)?;&#10;//!&#10;//! assert_eq!(record, restored);&#10;//! # Ok(())&#10;//! # }&#10;//! ```&#10;//!&#10;//! # Format Selection&#10;//!&#10;//! Use `SerializationFormatFactory::supported_formats()` to see available formats:&#10;//!&#10;//! ```rust&#10;//! use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;//!&#10;//! let formats = SerializationFormatFactory::supported_formats();&#10;//! println!(&quot;Available: {:?}&quot;, formats);&#10;//! // Output: [&quot;json&quot;, &quot;avro&quot;, &quot;protobuf&quot;, &quot;proto&quot;] (depending on features)&#10;//! ```&#10;//!&#10;//! # Schema-based Formats&#10;//!&#10;//! For Avro, you can provide custom schemas:&#10;//!&#10;//! ```rust,ignore&#10;//! #[cfg(feature = &quot;avro&quot;)]&#10;//! # fn avro_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {&#10;//! use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;//!&#10;//! let schema = r#&quot;&#10;//! {&#10;//!   &quot;type&quot;: &quot;record&quot;,&#10;//!   &quot;name&quot;: &quot;User&quot;,&#10;//!   &quot;fields&quot;: [&#10;//!     {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;long&quot;},&#10;//!     {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}&#10;//!   ]&#10;//! }&#10;//! &quot;#;&#10;//!&#10;//! let format = SerializationFormatFactory::create_avro_format(schema)?;&#10;//! # Ok(())&#10;//! # }&#10;//! ```&#10;&#10;use crate::ferris::sql::{FieldValue, SqlError};&#10;use std::collections::HashMap;&#10;&#10;/// Trait for pluggable serialization formats&#10;///&#10;/// This trait provides a consistent interface for different serialization formats&#10;/// (JSON, Avro, Protocol Buffers) used throughout FerrisStreams. All formats must&#10;/// support bidirectional conversion between external records and bytes, as well as&#10;/// conversion to/from the SQL execution engine's internal representation.&#10;///&#10;/// # Thread Safety&#10;///&#10;/// All implementations must be `Send + Sync` as format instances may be shared&#10;/// across threads in a streaming application.&#10;///&#10;/// # Example Implementation&#10;///&#10;/// ```rust,ignore&#10;/// struct MyFormat;&#10;///&#10;/// impl SerializationFormat for MyFormat {&#10;///     fn serialize_record(&amp;self, record: &amp;HashMap&lt;String, FieldValue&gt;) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;///         // Convert record to bytes using your format&#10;///         todo!()&#10;///     }&#10;///     &#10;///     fn deserialize_record(&amp;self, bytes: &amp;[u8]) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;///         // Convert bytes back to record&#10;///         todo!()&#10;///     }&#10;///     &#10;///     // ... other methods&#10;/// }&#10;/// ```&#10;pub trait SerializationFormat: Send + Sync {&#10;    /// Serialize a record to bytes for Kafka production&#10;    ///&#10;    /// Converts a typed record (HashMap&lt;String, FieldValue&gt;) into a byte array&#10;    /// suitable for storing in Kafka messages. The serialization format should&#10;    /// preserve all type information needed for accurate deserialization.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `record` - The typed record to serialize&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(Vec&lt;u8&gt;)` - Serialized bytes&#10;    /// * `Err(SerializationError)` - If serialization fails&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```rust,ignore&#10;    /// let mut record = HashMap::new();&#10;    /// record.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    ///&#10;    /// let bytes = format.serialize_record(&amp;record)?;&#10;    /// ```&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt;;&#10;&#10;    /// Deserialize bytes from Kafka into a record&#10;    ///&#10;    /// Converts bytes (typically from a Kafka message) back into a typed record.&#10;    /// Must be the inverse operation of `serialize_record`.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `bytes` - The serialized bytes to deserialize&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, FieldValue&gt;)` - Deserialized record&#10;    /// * `Err(SerializationError)` - If deserialization fails&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```rust,ignore&#10;    /// let record = format.deserialize_record(&amp;bytes)?;&#10;    /// println!(&quot;Name: {:?}&quot;, record.get(&quot;name&quot;));&#10;    /// ```&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt;;&#10;&#10;    /// Convert record to internal execution format&#10;    ///&#10;    /// Converts external FieldValue types to InternalValue types used by the SQL&#10;    /// execution engine. This enables the engine to process data regardless of the&#10;    /// original serialization format.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `record` - External format record&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, InternalValue&gt;)` - Internal format data&#10;    /// * `Err(SerializationError)` - If conversion fails&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt;;&#10;&#10;    /// Convert from internal execution format back to record&#10;    ///&#10;    /// Converts SQL engine InternalValue types back to external FieldValue types.&#10;    /// This is used when query results need to be serialized back to Kafka.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `data` - Internal format data from SQL engine&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, FieldValue&gt;)` - External format record&#10;    /// * `Err(SerializationError)` - If conversion fails&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt;;&#10;&#10;    /// Get the format name (for logging/debugging)&#10;    ///&#10;    /// Returns a human-readable name for this format. Used in logs, error messages,&#10;    /// and debugging output.&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// A static string identifying the format (e.g., &quot;JSON&quot;, &quot;Avro&quot;, &quot;Protobuf&quot;)&#10;    fn format_name(&amp;self) -&gt; &amp;'static str;&#10;}&#10;&#10;/// Internal value type for SQL execution (replaces hardcoded serde_json::Value)&#10;#[derive(Debug, Clone, PartialEq)]&#10;pub enum InternalValue {&#10;    String(String),&#10;    Number(f64),&#10;    Integer(i64),&#10;    Boolean(bool),&#10;    Null,&#10;    Array(Vec&lt;InternalValue&gt;),&#10;    Object(HashMap&lt;String, InternalValue&gt;),&#10;}&#10;&#10;/// Serialization error type&#10;#[derive(Debug)]&#10;pub enum SerializationError {&#10;    SerializationFailed(String),&#10;    DeserializationFailed(String),&#10;    FormatConversionFailed(String),&#10;    UnsupportedType(String),&#10;}&#10;&#10;impl std::fmt::Display for SerializationError {&#10;    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {&#10;        match self {&#10;            SerializationError::SerializationFailed(msg) =&gt; {&#10;                write!(f, &quot;Serialization failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::DeserializationFailed(msg) =&gt; {&#10;                write!(f, &quot;Deserialization failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::FormatConversionFailed(msg) =&gt; {&#10;                write!(f, &quot;Format conversion failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::UnsupportedType(msg) =&gt; write!(f, &quot;Unsupported type: {}&quot;, msg),&#10;        }&#10;    }&#10;}&#10;&#10;impl std::error::Error for SerializationError {}&#10;&#10;/// Convert SerializationError to SqlError&#10;impl From&lt;SerializationError&gt; for SqlError {&#10;    fn from(err: SerializationError) -&gt; Self {&#10;        SqlError::ExecutionError {&#10;            message: format!(&quot;Serialization error: {}&quot;, err),&#10;            query: None,&#10;        }&#10;    }&#10;}&#10;&#10;/// JSON implementation of SerializationFormat&#10;pub struct JsonFormat;&#10;&#10;impl SerializationFormat for JsonFormat {&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        // Convert FieldValue to serde_json::Value&#10;        let mut json_map = serde_json::Map::new();&#10;&#10;        for (key, field_value) in record {&#10;            let json_value = field_value_to_json(field_value)?;&#10;            json_map.insert(key.clone(), json_value);&#10;        }&#10;&#10;        let json_object = serde_json::Value::Object(json_map);&#10;        serde_json::to_vec(&amp;json_object)&#10;            .map_err(|e| SerializationError::SerializationFailed(e.to_string()))&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let json_value: serde_json::Value = serde_json::from_slice(bytes)&#10;            .map_err(|e| SerializationError::DeserializationFailed(e.to_string()))?;&#10;&#10;        match json_value {&#10;            serde_json::Value::Object(map) =&gt; {&#10;                let mut record = HashMap::new();&#10;                for (key, value) in map {&#10;                    let field_value = json_to_field_value(&amp;value)?;&#10;                    record.insert(key, field_value);&#10;                }&#10;                Ok(record)&#10;            }&#10;            _ =&gt; Err(SerializationError::DeserializationFailed(&#10;                &quot;Expected JSON object&quot;.to_string(),&#10;            )),&#10;        }&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;JSON&quot;&#10;    }&#10;}&#10;&#10;// Helper functions for JSON conversion&#10;&#10;fn field_value_to_json(field_value: &amp;FieldValue) -&gt; Result&lt;serde_json::Value, SerializationError&gt; {&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(serde_json::Value::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(serde_json::Value::Number(serde_json::Number::from(*i))),&#10;        FieldValue::Float(f) =&gt; serde_json::Number::from_f64(*f)&#10;            .map(serde_json::Value::Number)&#10;            .ok_or_else(|| {&#10;                SerializationError::FormatConversionFailed(format!(&quot;Invalid float: {}&quot;, f))&#10;            }),&#10;        FieldValue::Boolean(b) =&gt; Ok(serde_json::Value::Bool(*b)),&#10;        FieldValue::Null =&gt; Ok(serde_json::Value::Null),&#10;        FieldValue::Date(d) =&gt; Ok(serde_json::Value::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(serde_json::Value::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(serde_json::Value::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let json_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_json).collect();&#10;            Ok(serde_json::Value::Array(json_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut json_map = serde_json::Map::new();&#10;            for (k, v) in map {&#10;                json_map.insert(k.clone(), field_value_to_json(v)?);&#10;            }&#10;            Ok(serde_json::Value::Object(json_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut json_map = serde_json::Map::new();&#10;            for (k, v) in fields {&#10;                json_map.insert(k.clone(), field_value_to_json(v)?);&#10;            }&#10;            Ok(serde_json::Value::Object(json_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn json_to_field_value(json_value: &amp;serde_json::Value) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    match json_value {&#10;        serde_json::Value::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        serde_json::Value::Number(n) =&gt; {&#10;            if let Some(i) = n.as_i64() {&#10;                Ok(FieldValue::Integer(i))&#10;            } else if let Some(f) = n.as_f64() {&#10;                Ok(FieldValue::Float(f))&#10;            } else {&#10;                Ok(FieldValue::String(n.to_string()))&#10;            }&#10;        }&#10;        serde_json::Value::Bool(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        serde_json::Value::Null =&gt; Ok(FieldValue::Null),&#10;        serde_json::Value::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(json_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        serde_json::Value::Object(obj) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in obj {&#10;                field_map.insert(k.clone(), json_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn field_value_to_internal(field_value: &amp;FieldValue) -&gt; Result&lt;InternalValue, SerializationError&gt; {&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(InternalValue::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(InternalValue::Integer(*i)),&#10;        FieldValue::Float(f) =&gt; Ok(InternalValue::Number(*f)),&#10;        FieldValue::Boolean(b) =&gt; Ok(InternalValue::Boolean(*b)),&#10;        FieldValue::Null =&gt; Ok(InternalValue::Null),&#10;        FieldValue::Date(d) =&gt; Ok(InternalValue::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(InternalValue::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(InternalValue::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let internal_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_internal).collect();&#10;            Ok(InternalValue::Array(internal_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut internal_map = HashMap::new();&#10;            for (k, v) in map {&#10;                internal_map.insert(k.clone(), field_value_to_internal(v)?);&#10;            }&#10;            Ok(InternalValue::Object(internal_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut internal_map = HashMap::new();&#10;            for (k, v) in fields {&#10;                internal_map.insert(k.clone(), field_value_to_internal(v)?);&#10;            }&#10;            Ok(InternalValue::Object(internal_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn internal_to_field_value(&#10;    internal_value: &amp;InternalValue,&#10;) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    match internal_value {&#10;        InternalValue::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        InternalValue::Integer(i) =&gt; Ok(FieldValue::Integer(*i)),&#10;        InternalValue::Number(f) =&gt; Ok(FieldValue::Float(*f)),&#10;        InternalValue::Boolean(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        InternalValue::Null =&gt; Ok(FieldValue::Null),&#10;        InternalValue::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(internal_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        InternalValue::Object(obj) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in obj {&#10;                field_map.insert(k.clone(), internal_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;    }&#10;}&#10;&#10;// Avro serialization implementation (feature-gated)&#10;#[cfg(feature = &quot;avro&quot;)]&#10;pub struct AvroFormat {&#10;    writer_schema: apache_avro::Schema,&#10;    reader_schema: apache_avro::Schema,&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;impl AvroFormat {&#10;    /// Create new Avro format with schema&#10;    pub fn new(schema_json: &amp;str) -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let schema = apache_avro::Schema::parse_str(schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid Avro schema: {}&quot;, e))&#10;        })?;&#10;&#10;        Ok(AvroFormat {&#10;            writer_schema: schema.clone(),&#10;            reader_schema: schema,&#10;        })&#10;    }&#10;&#10;    /// Create Avro format with separate reader and writer schemas (for schema evolution)&#10;    pub fn with_schemas(&#10;        writer_schema_json: &amp;str,&#10;        reader_schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let writer_schema = apache_avro::Schema::parse_str(writer_schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid writer schema: {}&quot;, e))&#10;        })?;&#10;        let reader_schema = apache_avro::Schema::parse_str(reader_schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid reader schema: {}&quot;, e))&#10;        })?;&#10;&#10;        Ok(AvroFormat {&#10;            writer_schema,&#10;            reader_schema,&#10;        })&#10;    }&#10;&#10;    /// Create a default Avro format with generic record schema&#10;    pub fn default_format() -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let schema_json = r#&quot;&#10;        {&#10;            &quot;type&quot;: &quot;record&quot;,&#10;            &quot;name&quot;: &quot;GenericRecord&quot;,&#10;            &quot;fields&quot;: [&#10;                {&quot;name&quot;: &quot;data&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;, &quot;long&quot;, &quot;double&quot;, &quot;boolean&quot;, {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}]}&#10;            ]&#10;        }&#10;        &quot;#;&#10;        Self::new(schema_json)&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;impl SerializationFormat for AvroFormat {&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        use apache_avro::Writer;&#10;&#10;        // Convert record to Avro value&#10;        let avro_value = record_to_avro_value(record)?;&#10;&#10;        // Create writer and encode&#10;        let mut writer = Writer::new(&amp;self.writer_schema, Vec::new());&#10;        writer.append(avro_value).map_err(|e| {&#10;            SerializationError::SerializationFailed(format!(&quot;Avro serialization failed: {}&quot;, e))&#10;        })?;&#10;&#10;        writer.into_inner().map_err(|e| {&#10;            SerializationError::SerializationFailed(format!(&#10;                &quot;Avro writer finalization failed: {}&quot;,&#10;                e&#10;            ))&#10;        })&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        use apache_avro::Reader;&#10;&#10;        let mut reader = Reader::with_schema(&amp;self.reader_schema, bytes).map_err(|e| {&#10;            SerializationError::DeserializationFailed(format!(&quot;Avro reader creation failed: {}&quot;, e))&#10;        })?;&#10;&#10;        // Read first record (assuming single record per message)&#10;        if let Some(record_result) = reader.next() {&#10;            let avro_value = record_result.map_err(|e| {&#10;                SerializationError::DeserializationFailed(format!(&#10;                    &quot;Avro deserialization failed: {}&quot;,&#10;                    e&#10;                ))&#10;            })?;&#10;&#10;            return avro_value_to_record(&amp;avro_value);&#10;        }&#10;&#10;        Err(SerializationError::DeserializationFailed(&#10;            &quot;No records found in Avro data&quot;.to_string(),&#10;        ))&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;Avro&quot;&#10;    }&#10;}&#10;&#10;// Avro conversion helpers&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn record_to_avro_value(&#10;    record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;) -&gt; Result&lt;apache_avro::types::Value, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    let mut avro_fields = Vec::new();&#10;    for (key, field_value) in record {&#10;        let avro_value = match field_value {&#10;            FieldValue::Null =&gt; {&#10;                // For nullable fields in unions, wrap null in union with index 1&#10;                // (assuming [&quot;string&quot;, &quot;null&quot;] or similar patterns where null is typically second)&#10;                Value::Union(1, Box::new(Value::Null))&#10;            }&#10;            _ =&gt; field_value_to_avro(field_value)?,&#10;        };&#10;        avro_fields.push((key.clone(), avro_value));&#10;    }&#10;&#10;    Ok(Value::Record(avro_fields))&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn field_value_to_avro(&#10;    field_value: &amp;FieldValue,&#10;) -&gt; Result&lt;apache_avro::types::Value, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(Value::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(Value::Long(*i)),&#10;        FieldValue::Float(f) =&gt; Ok(Value::Double(*f)),&#10;        FieldValue::Boolean(b) =&gt; Ok(Value::Boolean(*b)),&#10;        FieldValue::Null =&gt; Ok(Value::Null),&#10;        FieldValue::Date(d) =&gt; Ok(Value::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(Value::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(Value::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let avro_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_avro).collect();&#10;            Ok(Value::Array(avro_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut avro_map = std::collections::HashMap::new();&#10;            for (k, v) in map {&#10;                avro_map.insert(k.clone(), field_value_to_avro(v)?);&#10;            }&#10;            Ok(Value::Map(avro_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut avro_fields = Vec::new();&#10;            for (k, v) in fields {&#10;                avro_fields.push((k.clone(), field_value_to_avro(v)?));&#10;            }&#10;            Ok(Value::Record(avro_fields))&#10;        }&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn avro_value_to_record(&#10;    avro_value: &amp;apache_avro::types::Value,&#10;) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;    match avro_value {&#10;        apache_avro::types::Value::Record(fields) =&gt; {&#10;            let mut record = HashMap::new();&#10;            for (key, value) in fields {&#10;                let field_value = avro_value_to_field_value(value)?;&#10;                record.insert(key.clone(), field_value);&#10;            }&#10;            Ok(record)&#10;        }&#10;        _ =&gt; Err(SerializationError::DeserializationFailed(&#10;            &quot;Expected Avro record&quot;.to_string(),&#10;        )),&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn avro_value_to_field_value(&#10;    avro_value: &amp;apache_avro::types::Value,&#10;) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    match avro_value {&#10;        Value::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        Value::Long(i) =&gt; Ok(FieldValue::Integer(*i)),&#10;        Value::Int(i) =&gt; Ok(FieldValue::Integer(*i as i64)),&#10;        Value::Float(f) =&gt; Ok(FieldValue::Float(*f as f64)),&#10;        Value::Double(f) =&gt; Ok(FieldValue::Float(*f)),&#10;        Value::Boolean(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        Value::Null =&gt; Ok(FieldValue::Null),&#10;        Value::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(avro_value_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        Value::Map(map) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in map {&#10;                field_map.insert(k.clone(), avro_value_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;        Value::Record(fields) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in fields {&#10;                field_map.insert(k.clone(), avro_value_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Struct(field_map))&#10;        }&#10;        Value::Union(_, boxed_value) =&gt; avro_value_to_field_value(boxed_value),&#10;        _ =&gt; Err(SerializationError::UnsupportedType(format!(&#10;            &quot;Unsupported Avro type: {:?}&quot;,&#10;            avro_value&#10;        ))),&#10;    }&#10;}&#10;&#10;// Protocol Buffers serialization implementation (feature-gated)&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;pub struct ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    _phantom: std::marker::PhantomData&lt;T&gt;,&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; Default for ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    fn default() -&gt; Self {&#10;        Self::new()&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    /// Create new Protobuf format&#10;    pub fn new() -&gt; Self {&#10;        ProtobufFormat {&#10;            _phantom: std::marker::PhantomData,&#10;        }&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; SerializationFormat for ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default + Clone + Send + Sync + 'static,&#10;{&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        // For generic protobuf support, we'll encode as a map-like structure&#10;        // In practice, you'd want to use specific message types&#10;        let mut buf = Vec::new();&#10;&#10;        // Convert record to JSON first, then to bytes (simplified approach)&#10;        let json_value = record_to_json_map(record)?;&#10;        let json_bytes = serde_json::to_vec(&amp;json_value)&#10;            .map_err(|e| SerializationError::SerializationFailed(e.to_string()))?;&#10;&#10;        // For a generic implementation, we'll store JSON as bytes&#10;        // Real implementation would use proper protobuf message definitions&#10;        buf.extend_from_slice(&amp;json_bytes);&#10;        Ok(buf)&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        // For generic protobuf support, decode from JSON bytes&#10;        let json_value: serde_json::Value = serde_json::from_slice(bytes)&#10;            .map_err(|e| SerializationError::DeserializationFailed(e.to_string()))?;&#10;&#10;        match json_value {&#10;            serde_json::Value::Object(map) =&gt; {&#10;                let mut record = HashMap::new();&#10;                for (key, value) in map {&#10;                    let field_value = json_to_field_value(&amp;value)?;&#10;                    record.insert(key, field_value);&#10;                }&#10;                Ok(record)&#10;            }&#10;            _ =&gt; Err(SerializationError::DeserializationFailed(&#10;                &quot;Expected JSON object from protobuf data&quot;.to_string(),&#10;            )),&#10;        }&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;Protobuf&quot;&#10;    }&#10;}&#10;&#10;// Helper for protobuf conversion&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;fn record_to_json_map(&#10;    record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;) -&gt; Result&lt;serde_json::Map&lt;String, serde_json::Value&gt;, SerializationError&gt; {&#10;    let mut json_map = serde_json::Map::new();&#10;&#10;    for (key, field_value) in record {&#10;        let json_value = field_value_to_json(field_value)?;&#10;        json_map.insert(key.clone(), json_value);&#10;    }&#10;&#10;    Ok(json_map)&#10;}&#10;&#10;/// Factory for creating serialization formats&#10;///&#10;/// This factory provides a centralized way to create serialization format instances&#10;/// by name or with custom configuration. It handles feature detection and returns&#10;/// appropriate errors for unsupported formats.&#10;///&#10;/// # Examples&#10;///&#10;/// ```rust&#10;/// use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;///&#10;/// // Create JSON format (always available)&#10;/// let json = SerializationFormatFactory::create_format(&quot;json&quot;).unwrap();&#10;///&#10;/// // Get list of supported formats&#10;/// let formats = SerializationFormatFactory::supported_formats();&#10;/// println!(&quot;Available: {:?}&quot;, formats);&#10;///&#10;/// // Get default format&#10;/// let default = SerializationFormatFactory::default_format();&#10;/// ```&#10;pub struct SerializationFormatFactory;&#10;&#10;impl SerializationFormatFactory {&#10;    /// Create a serialization format by name&#10;    ///&#10;    /// Creates a format instance using the default configuration for the specified&#10;    /// format. For schema-based formats like Avro, this uses a generic schema.&#10;    ///&#10;    /// # Supported Format Names&#10;    ///&#10;    /// - `&quot;json&quot;` - Always available&#10;    /// - `&quot;avro&quot;` - Requires `avro` feature, uses default schema&#10;    /// - `&quot;protobuf&quot;` or `&quot;proto&quot;` - Requires `protobuf` feature&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `format_name` - Case-insensitive format name&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(Box&lt;dyn SerializationFormat&gt;)` - Format instance&#10;    /// * `Err(SerializationError::UnsupportedType)` - If format is unknown or feature not enabled&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```rust&#10;    /// use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;    ///&#10;    /// let json_format = SerializationFormatFactory::create_format(&quot;json&quot;)?;&#10;    /// assert_eq!(json_format.format_name(), &quot;JSON&quot;);&#10;    /// # Ok::&lt;(), Box&lt;dyn std::error::Error&gt;&gt;(())&#10;    /// ```&#10;    pub fn create_format(&#10;        format_name: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        match format_name.to_lowercase().as_str() {&#10;            &quot;json&quot; =&gt; Ok(Box::new(JsonFormat)),&#10;            #[cfg(feature = &quot;avro&quot;)]&#10;            &quot;avro&quot; =&gt; {&#10;                let avro_format = AvroFormat::default_format().map_err(|e| {&#10;                    SerializationError::FormatConversionFailed(format!(&#10;                        &quot;Failed to create Avro format: {}&quot;,&#10;                        e&#10;                    ))&#10;                })?;&#10;                Ok(Box::new(avro_format))&#10;            }&#10;            #[cfg(feature = &quot;protobuf&quot;)]&#10;            &quot;protobuf&quot; | &quot;proto&quot; =&gt; {&#10;                // Generic protobuf format - in practice you'd specify the message type&#10;                Ok(Box::new(ProtobufFormat::&lt;()&gt;::new()))&#10;            }&#10;            _ =&gt; Err(SerializationError::UnsupportedType(format!(&#10;                &quot;Unknown format: {}&quot;,&#10;                format_name&#10;            ))),&#10;        }&#10;    }&#10;&#10;    /// Create Avro format with custom schema&#10;    #[cfg(feature = &quot;avro&quot;)]&#10;    pub fn create_avro_format(&#10;        schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        let avro_format = AvroFormat::new(schema_json)?;&#10;        Ok(Box::new(avro_format))&#10;    }&#10;&#10;    /// Create Avro format with schema evolution support&#10;    #[cfg(feature = &quot;avro&quot;)]&#10;    pub fn create_avro_format_with_schemas(&#10;        writer_schema_json: &amp;str,&#10;        reader_schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        let avro_format = AvroFormat::with_schemas(writer_schema_json, reader_schema_json)?;&#10;        Ok(Box::new(avro_format))&#10;    }&#10;&#10;    /// Create Protobuf format with specific message type&#10;    #[cfg(feature = &quot;protobuf&quot;)]&#10;    pub fn create_protobuf_format&lt;T&gt;() -&gt; Box&lt;dyn SerializationFormat&gt;&#10;    where&#10;        T: prost::Message + Default + Clone + Send + Sync + 'static,&#10;    {&#10;        Box::new(ProtobufFormat::&lt;T&gt;::new())&#10;    }&#10;&#10;    /// Get list of supported formats&#10;    pub fn supported_formats() -&gt; Vec&lt;&amp;'static str&gt; {&#10;        let mut formats = vec![&quot;json&quot;];&#10;&#10;        #[cfg(feature = &quot;avro&quot;)]&#10;        formats.push(&quot;avro&quot;);&#10;&#10;        #[cfg(feature = &quot;protobuf&quot;)]&#10;        {&#10;            formats.push(&quot;protobuf&quot;);&#10;            formats.push(&quot;proto&quot;);&#10;        }&#10;&#10;        formats&#10;    }&#10;&#10;    /// Get default format (JSON)&#10;    pub fn default_format() -&gt; Box&lt;dyn SerializationFormat&gt; {&#10;        Box::new(JsonFormat)&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="//! Pluggable serialization interface for FerrisStreams&#10;//!&#10;//! This module provides a comprehensive trait-based approach to serialization that allows&#10;//! the system to work with multiple data formats (JSON, Avro, Protobuf, etc.) instead&#10;//! of being hardcoded to JSON.&#10;//!&#10;//! # Features&#10;//!&#10;//! - **JSON**: Always available, human-readable, good for development&#10;//! - **Avro**: Feature-gated (`avro`), schema-based, supports evolution&#10;//! - **Protobuf**: Feature-gated (`protobuf`), very compact, high performance&#10;//!&#10;//! # Quick Start&#10;//!&#10;//! ```no_run&#10;//! use ferrisstreams::ferris::serialization::{SerializationFormatFactory, FieldValue};&#10;//! use std::collections::HashMap;&#10;//!&#10;//! # fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {&#10;//! // Create a format&#10;//! let format = SerializationFormatFactory::create_format(&quot;json&quot;)?;&#10;//!&#10;//! // Create a record&#10;//! let mut record = HashMap::new();&#10;//! record.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;//! record.insert(&quot;age&quot;.to_string(), FieldValue::Integer(30));&#10;//!&#10;//! // Serialize and deserialize&#10;//! let bytes = format.serialize_record(&amp;record)?;&#10;//! let restored = format.deserialize_record(&amp;bytes)?;&#10;//!&#10;//! assert_eq!(record, restored);&#10;//! # Ok(())&#10;//! # }&#10;//! ```&#10;//!&#10;//! # Format Selection&#10;//!&#10;//! Use `SerializationFormatFactory::supported_formats()` to see available formats:&#10;//!&#10;//! ```no_run&#10;//! use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;//!&#10;//! let formats = SerializationFormatFactory::supported_formats();&#10;//! println!(&quot;Available: {:?}&quot;, formats);&#10;//! // Output: [&quot;json&quot;, &quot;avro&quot;, &quot;protobuf&quot;, &quot;proto&quot;] (depending on features)&#10;//! ```&#10;//!&#10;//! # Schema-based Formats&#10;//!&#10;//! For Avro, you can provide custom schemas:&#10;//!&#10;//! ```ignore&#10;//! #[cfg(feature = &quot;avro&quot;)]&#10;//! # fn avro_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {&#10;//! use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;//!&#10;//! let schema = r#&quot;&#10;//! {&#10;//!   &quot;type&quot;: &quot;record&quot;,&#10;//!   &quot;name&quot;: &quot;User&quot;,&#10;//!   &quot;fields&quot;: [&#10;//!     {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;long&quot;},&#10;//!     {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}&#10;//!   ]&#10;//! }&#10;//! &quot;#;&#10;//!&#10;//! let format = SerializationFormatFactory::create_avro_format(schema)?;&#10;//! # Ok(())&#10;//! # }&#10;//! ```&#10;&#10;use crate::ferris::sql::{FieldValue, SqlError};&#10;use std::collections::HashMap;&#10;&#10;/// Trait for pluggable serialization formats&#10;///&#10;/// This trait provides a consistent interface for different serialization formats&#10;/// (JSON, Avro, Protocol Buffers) used throughout FerrisStreams. All formats must&#10;/// support bidirectional conversion between external records and bytes, as well as&#10;/// conversion to/from the SQL execution engine's internal representation.&#10;///&#10;/// # Thread Safety&#10;///&#10;/// All implementations must be `Send + Sync` as format instances may be shared&#10;/// across threads in a streaming application.&#10;///&#10;/// # Example Implementation&#10;///&#10;/// ```ignore&#10;/// struct MyFormat;&#10;///&#10;/// impl SerializationFormat for MyFormat {&#10;///     fn serialize_record(&amp;self, record: &amp;HashMap&lt;String, FieldValue&gt;) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;///         // Convert record to bytes using your format&#10;///         todo!()&#10;///     }&#10;///     &#10;///     fn deserialize_record(&amp;self, bytes: &amp;[u8]) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;///         // Convert bytes back to record&#10;///         todo!()&#10;///     }&#10;///     &#10;///     // ... other methods&#10;/// }&#10;/// ```&#10;pub trait SerializationFormat: Send + Sync {&#10;    /// Serialize a record to bytes for Kafka production&#10;    ///&#10;    /// Converts a typed record (HashMap&lt;String, FieldValue&gt;) into a byte array&#10;    /// suitable for storing in Kafka messages. The serialization format should&#10;    /// preserve all type information needed for accurate deserialization.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `record` - The typed record to serialize&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(Vec&lt;u8&gt;)` - Serialized bytes&#10;    /// * `Err(SerializationError)` - If serialization fails&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```rust,ignore&#10;    /// let mut record = HashMap::new();&#10;    /// record.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    ///&#10;    /// let bytes = format.serialize_record(&amp;record)?;&#10;    /// ```&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt;;&#10;&#10;    /// Deserialize bytes from Kafka into a record&#10;    ///&#10;    /// Converts bytes (typically from a Kafka message) back into a typed record.&#10;    /// Must be the inverse operation of `serialize_record`.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `bytes` - The serialized bytes to deserialize&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, FieldValue&gt;)` - Deserialized record&#10;    /// * `Err(SerializationError)` - If deserialization fails&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```rust,ignore&#10;    /// let record = format.deserialize_record(&amp;bytes)?;&#10;    /// println!(&quot;Name: {:?}&quot;, record.get(&quot;name&quot;));&#10;    /// ```&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt;;&#10;&#10;    /// Convert record to internal execution format&#10;    ///&#10;    /// Converts external FieldValue types to InternalValue types used by the SQL&#10;    /// execution engine. This enables the engine to process data regardless of the&#10;    /// original serialization format.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `record` - External format record&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, InternalValue&gt;)` - Internal format data&#10;    /// * `Err(SerializationError)` - If conversion fails&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt;;&#10;&#10;    /// Convert from internal execution format back to record&#10;    ///&#10;    /// Converts SQL engine InternalValue types back to external FieldValue types.&#10;    /// This is used when query results need to be serialized back to Kafka.&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `data` - Internal format data from SQL engine&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(HashMap&lt;String, FieldValue&gt;)` - External format record&#10;    /// * `Err(SerializationError)` - If conversion fails&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt;;&#10;&#10;    /// Get the format name (for logging/debugging)&#10;    ///&#10;    /// Returns a human-readable name for this format. Used in logs, error messages,&#10;    /// and debugging output.&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// A static string identifying the format (e.g., &quot;JSON&quot;, &quot;Avro&quot;, &quot;Protobuf&quot;)&#10;    fn format_name(&amp;self) -&gt; &amp;'static str;&#10;}&#10;&#10;/// Internal value type for SQL execution (replaces hardcoded serde_json::Value)&#10;#[derive(Debug, Clone, PartialEq)]&#10;pub enum InternalValue {&#10;    String(String),&#10;    Number(f64),&#10;    Integer(i64),&#10;    Boolean(bool),&#10;    Null,&#10;    Array(Vec&lt;InternalValue&gt;),&#10;    Object(HashMap&lt;String, InternalValue&gt;),&#10;}&#10;&#10;/// Serialization error type&#10;#[derive(Debug)]&#10;pub enum SerializationError {&#10;    SerializationFailed(String),&#10;    DeserializationFailed(String),&#10;    FormatConversionFailed(String),&#10;    UnsupportedType(String),&#10;}&#10;&#10;impl std::fmt::Display for SerializationError {&#10;    fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {&#10;        match self {&#10;            SerializationError::SerializationFailed(msg) =&gt; {&#10;                write!(f, &quot;Serialization failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::DeserializationFailed(msg) =&gt; {&#10;                write!(f, &quot;Deserialization failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::FormatConversionFailed(msg) =&gt; {&#10;                write!(f, &quot;Format conversion failed: {}&quot;, msg)&#10;            }&#10;            SerializationError::UnsupportedType(msg) =&gt; write!(f, &quot;Unsupported type: {}&quot;, msg),&#10;        }&#10;    }&#10;}&#10;&#10;impl std::error::Error for SerializationError {}&#10;&#10;/// Convert SerializationError to SqlError&#10;impl From&lt;SerializationError&gt; for SqlError {&#10;    fn from(err: SerializationError) -&gt; Self {&#10;        SqlError::ExecutionError {&#10;            message: format!(&quot;Serialization error: {}&quot;, err),&#10;            query: None,&#10;        }&#10;    }&#10;}&#10;&#10;/// JSON implementation of SerializationFormat&#10;pub struct JsonFormat;&#10;&#10;impl SerializationFormat for JsonFormat {&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        // Convert FieldValue to serde_json::Value&#10;        let mut json_map = serde_json::Map::new();&#10;&#10;        for (key, field_value) in record {&#10;            let json_value = field_value_to_json(field_value)?;&#10;            json_map.insert(key.clone(), json_value);&#10;        }&#10;&#10;        let json_object = serde_json::Value::Object(json_map);&#10;        serde_json::to_vec(&amp;json_object)&#10;            .map_err(|e| SerializationError::SerializationFailed(e.to_string()))&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let json_value: serde_json::Value = serde_json::from_slice(bytes)&#10;            .map_err(|e| SerializationError::DeserializationFailed(e.to_string()))?;&#10;&#10;        match json_value {&#10;            serde_json::Value::Object(map) =&gt; {&#10;                let mut record = HashMap::new();&#10;                for (key, value) in map {&#10;                    let field_value = json_to_field_value(&amp;value)?;&#10;                    record.insert(key, field_value);&#10;                }&#10;                Ok(record)&#10;            }&#10;            _ =&gt; Err(SerializationError::DeserializationFailed(&#10;                &quot;Expected JSON object&quot;.to_string(),&#10;            )),&#10;        }&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;JSON&quot;&#10;    }&#10;}&#10;&#10;// Helper functions for JSON conversion&#10;&#10;fn field_value_to_json(field_value: &amp;FieldValue) -&gt; Result&lt;serde_json::Value, SerializationError&gt; {&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(serde_json::Value::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(serde_json::Value::Number(serde_json::Number::from(*i))),&#10;        FieldValue::Float(f) =&gt; serde_json::Number::from_f64(*f)&#10;            .map(serde_json::Value::Number)&#10;            .ok_or_else(|| {&#10;                SerializationError::FormatConversionFailed(format!(&quot;Invalid float: {}&quot;, f))&#10;            }),&#10;        FieldValue::Boolean(b) =&gt; Ok(serde_json::Value::Bool(*b)),&#10;        FieldValue::Null =&gt; Ok(serde_json::Value::Null),&#10;        FieldValue::Date(d) =&gt; Ok(serde_json::Value::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(serde_json::Value::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(serde_json::Value::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let json_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_json).collect();&#10;            Ok(serde_json::Value::Array(json_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut json_map = serde_json::Map::new();&#10;            for (k, v) in map {&#10;                json_map.insert(k.clone(), field_value_to_json(v)?);&#10;            }&#10;            Ok(serde_json::Value::Object(json_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut json_map = serde_json::Map::new();&#10;            for (k, v) in fields {&#10;                json_map.insert(k.clone(), field_value_to_json(v)?);&#10;            }&#10;            Ok(serde_json::Value::Object(json_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn json_to_field_value(json_value: &amp;serde_json::Value) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    match json_value {&#10;        serde_json::Value::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        serde_json::Value::Number(n) =&gt; {&#10;            if let Some(i) = n.as_i64() {&#10;                Ok(FieldValue::Integer(i))&#10;            } else if let Some(f) = n.as_f64() {&#10;                Ok(FieldValue::Float(f))&#10;            } else {&#10;                Ok(FieldValue::String(n.to_string()))&#10;            }&#10;        }&#10;        serde_json::Value::Bool(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        serde_json::Value::Null =&gt; Ok(FieldValue::Null),&#10;        serde_json::Value::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(json_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        serde_json::Value::Object(obj) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in obj {&#10;                field_map.insert(k.clone(), json_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn field_value_to_internal(field_value: &amp;FieldValue) -&gt; Result&lt;InternalValue, SerializationError&gt; {&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(InternalValue::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(InternalValue::Integer(*i)),&#10;        FieldValue::Float(f) =&gt; Ok(InternalValue::Number(*f)),&#10;        FieldValue::Boolean(b) =&gt; Ok(InternalValue::Boolean(*b)),&#10;        FieldValue::Null =&gt; Ok(InternalValue::Null),&#10;        FieldValue::Date(d) =&gt; Ok(InternalValue::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(InternalValue::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(InternalValue::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let internal_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_internal).collect();&#10;            Ok(InternalValue::Array(internal_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut internal_map = HashMap::new();&#10;            for (k, v) in map {&#10;                internal_map.insert(k.clone(), field_value_to_internal(v)?);&#10;            }&#10;            Ok(InternalValue::Object(internal_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut internal_map = HashMap::new();&#10;            for (k, v) in fields {&#10;                internal_map.insert(k.clone(), field_value_to_internal(v)?);&#10;            }&#10;            Ok(InternalValue::Object(internal_map))&#10;        }&#10;    }&#10;}&#10;&#10;fn internal_to_field_value(&#10;    internal_value: &amp;InternalValue,&#10;) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    match internal_value {&#10;        InternalValue::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        InternalValue::Integer(i) =&gt; Ok(FieldValue::Integer(*i)),&#10;        InternalValue::Number(f) =&gt; Ok(FieldValue::Float(*f)),&#10;        InternalValue::Boolean(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        InternalValue::Null =&gt; Ok(FieldValue::Null),&#10;        InternalValue::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(internal_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        InternalValue::Object(obj) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in obj {&#10;                field_map.insert(k.clone(), internal_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;    }&#10;}&#10;&#10;// Avro serialization implementation (feature-gated)&#10;#[cfg(feature = &quot;avro&quot;)]&#10;pub struct AvroFormat {&#10;    writer_schema: apache_avro::Schema,&#10;    reader_schema: apache_avro::Schema,&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;impl AvroFormat {&#10;    /// Create new Avro format with schema&#10;    pub fn new(schema_json: &amp;str) -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let schema = apache_avro::Schema::parse_str(schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid Avro schema: {}&quot;, e))&#10;        })?;&#10;&#10;        Ok(AvroFormat {&#10;            writer_schema: schema.clone(),&#10;            reader_schema: schema,&#10;        })&#10;    }&#10;&#10;    /// Create Avro format with separate reader and writer schemas (for schema evolution)&#10;    pub fn with_schemas(&#10;        writer_schema_json: &amp;str,&#10;        reader_schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let writer_schema = apache_avro::Schema::parse_str(writer_schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid writer schema: {}&quot;, e))&#10;        })?;&#10;        let reader_schema = apache_avro::Schema::parse_str(reader_schema_json).map_err(|e| {&#10;            SerializationError::FormatConversionFailed(format!(&quot;Invalid reader schema: {}&quot;, e))&#10;        })?;&#10;&#10;        Ok(AvroFormat {&#10;            writer_schema,&#10;            reader_schema,&#10;        })&#10;    }&#10;&#10;    /// Create a default Avro format with generic record schema&#10;    pub fn default_format() -&gt; Result&lt;Self, SerializationError&gt; {&#10;        let schema_json = r#&quot;&#10;        {&#10;            &quot;type&quot;: &quot;record&quot;,&#10;            &quot;name&quot;: &quot;GenericRecord&quot;,&#10;            &quot;fields&quot;: [&#10;                {&quot;name&quot;: &quot;data&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;, &quot;long&quot;, &quot;double&quot;, &quot;boolean&quot;, {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}]}&#10;            ]&#10;        }&#10;        &quot;#;&#10;        Self::new(schema_json)&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;impl SerializationFormat for AvroFormat {&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        use apache_avro::Writer;&#10;&#10;        // Convert record to Avro value&#10;        let avro_value = record_to_avro_value(record)?;&#10;&#10;        // Create writer and encode&#10;        let mut writer = Writer::new(&amp;self.writer_schema, Vec::new());&#10;        writer.append(avro_value).map_err(|e| {&#10;            SerializationError::SerializationFailed(format!(&quot;Avro serialization failed: {}&quot;, e))&#10;        })?;&#10;&#10;        writer.into_inner().map_err(|e| {&#10;            SerializationError::SerializationFailed(format!(&#10;                &quot;Avro writer finalization failed: {}&quot;,&#10;                e&#10;            ))&#10;        })&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        use apache_avro::Reader;&#10;&#10;        let mut reader = Reader::with_schema(&amp;self.reader_schema, bytes).map_err(|e| {&#10;            SerializationError::DeserializationFailed(format!(&quot;Avro reader creation failed: {}&quot;, e))&#10;        })?;&#10;&#10;        // Read first record (assuming single record per message)&#10;        if let Some(record_result) = reader.next() {&#10;            let avro_value = record_result.map_err(|e| {&#10;                SerializationError::DeserializationFailed(format!(&#10;                    &quot;Avro deserialization failed: {}&quot;,&#10;                    e&#10;                ))&#10;            })?;&#10;&#10;            return avro_value_to_record(&amp;avro_value);&#10;        }&#10;&#10;        Err(SerializationError::DeserializationFailed(&#10;            &quot;No records found in Avro data&quot;.to_string(),&#10;        ))&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;Avro&quot;&#10;    }&#10;}&#10;&#10;// Avro conversion helpers&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn record_to_avro_value(&#10;    record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;) -&gt; Result&lt;apache_avro::types::Value, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    let mut avro_fields = Vec::new();&#10;    for (key, field_value) in record {&#10;        let avro_value = match field_value {&#10;            FieldValue::Null =&gt; {&#10;                // For nullable fields in unions, wrap null in union with index 1&#10;                // (assuming [&quot;string&quot;, &quot;null&quot;] or similar patterns where null is typically second)&#10;                Value::Union(1, Box::new(Value::Null))&#10;            }&#10;            _ =&gt; field_value_to_avro(field_value)?,&#10;        };&#10;        avro_fields.push((key.clone(), avro_value));&#10;    }&#10;&#10;    Ok(Value::Record(avro_fields))&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn field_value_to_avro(&#10;    field_value: &amp;FieldValue,&#10;) -&gt; Result&lt;apache_avro::types::Value, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    match field_value {&#10;        FieldValue::String(s) =&gt; Ok(Value::String(s.clone())),&#10;        FieldValue::Integer(i) =&gt; Ok(Value::Long(*i)),&#10;        FieldValue::Float(f) =&gt; Ok(Value::Double(*f)),&#10;        FieldValue::Boolean(b) =&gt; Ok(Value::Boolean(*b)),&#10;        FieldValue::Null =&gt; Ok(Value::Null),&#10;        FieldValue::Date(d) =&gt; Ok(Value::String(d.format(&quot;%Y-%m-%d&quot;).to_string())),&#10;        FieldValue::Timestamp(ts) =&gt; Ok(Value::String(&#10;            ts.format(&quot;%Y-%m-%d %H:%M:%S%.3f&quot;).to_string(),&#10;        )),&#10;        FieldValue::Decimal(dec) =&gt; Ok(Value::String(dec.to_string())),&#10;        FieldValue::Array(arr) =&gt; {&#10;            let avro_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(field_value_to_avro).collect();&#10;            Ok(Value::Array(avro_arr?))&#10;        }&#10;        FieldValue::Map(map) =&gt; {&#10;            let mut avro_map = std::collections::HashMap::new();&#10;            for (k, v) in map {&#10;                avro_map.insert(k.clone(), field_value_to_avro(v)?);&#10;            }&#10;            Ok(Value::Map(avro_map))&#10;        }&#10;        FieldValue::Struct(fields) =&gt; {&#10;            let mut avro_fields = Vec::new();&#10;            for (k, v) in fields {&#10;                avro_fields.push((k.clone(), field_value_to_avro(v)?));&#10;            }&#10;            Ok(Value::Record(avro_fields))&#10;        }&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn avro_value_to_record(&#10;    avro_value: &amp;apache_avro::types::Value,&#10;) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;    match avro_value {&#10;        apache_avro::types::Value::Record(fields) =&gt; {&#10;            let mut record = HashMap::new();&#10;            for (key, value) in fields {&#10;                let field_value = avro_value_to_field_value(value)?;&#10;                record.insert(key.clone(), field_value);&#10;            }&#10;            Ok(record)&#10;        }&#10;        _ =&gt; Err(SerializationError::DeserializationFailed(&#10;            &quot;Expected Avro record&quot;.to_string(),&#10;        )),&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;avro&quot;)]&#10;fn avro_value_to_field_value(&#10;    avro_value: &amp;apache_avro::types::Value,&#10;) -&gt; Result&lt;FieldValue, SerializationError&gt; {&#10;    use apache_avro::types::Value;&#10;&#10;    match avro_value {&#10;        Value::String(s) =&gt; Ok(FieldValue::String(s.clone())),&#10;        Value::Long(i) =&gt; Ok(FieldValue::Integer(*i)),&#10;        Value::Int(i) =&gt; Ok(FieldValue::Integer(*i as i64)),&#10;        Value::Float(f) =&gt; Ok(FieldValue::Float(*f as f64)),&#10;        Value::Double(f) =&gt; Ok(FieldValue::Float(*f)),&#10;        Value::Boolean(b) =&gt; Ok(FieldValue::Boolean(*b)),&#10;        Value::Null =&gt; Ok(FieldValue::Null),&#10;        Value::Array(arr) =&gt; {&#10;            let field_arr: Result&lt;Vec&lt;_&gt;, _&gt; = arr.iter().map(avro_value_to_field_value).collect();&#10;            Ok(FieldValue::Array(field_arr?))&#10;        }&#10;        Value::Map(map) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in map {&#10;                field_map.insert(k.clone(), avro_value_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Map(field_map))&#10;        }&#10;        Value::Record(fields) =&gt; {&#10;            let mut field_map = HashMap::new();&#10;            for (k, v) in fields {&#10;                field_map.insert(k.clone(), avro_value_to_field_value(v)?);&#10;            }&#10;            Ok(FieldValue::Struct(field_map))&#10;        }&#10;        Value::Union(_, boxed_value) =&gt; avro_value_to_field_value(boxed_value),&#10;        _ =&gt; Err(SerializationError::UnsupportedType(format!(&#10;            &quot;Unsupported Avro type: {:?}&quot;,&#10;            avro_value&#10;        ))),&#10;    }&#10;}&#10;&#10;// Protocol Buffers serialization implementation (feature-gated)&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;pub struct ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    _phantom: std::marker::PhantomData&lt;T&gt;,&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; Default for ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    fn default() -&gt; Self {&#10;        Self::new()&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default,&#10;{&#10;    /// Create new Protobuf format&#10;    pub fn new() -&gt; Self {&#10;        ProtobufFormat {&#10;            _phantom: std::marker::PhantomData,&#10;        }&#10;    }&#10;}&#10;&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;impl&lt;T&gt; SerializationFormat for ProtobufFormat&lt;T&gt;&#10;where&#10;    T: prost::Message + Default + Clone + Send + Sync + 'static,&#10;{&#10;    fn serialize_record(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;Vec&lt;u8&gt;, SerializationError&gt; {&#10;        // For generic protobuf support, we'll encode as a map-like structure&#10;        // In practice, you'd want to use specific message types&#10;        let mut buf = Vec::new();&#10;&#10;        // Convert record to JSON first, then to bytes (simplified approach)&#10;        let json_value = record_to_json_map(record)?;&#10;        let json_bytes = serde_json::to_vec(&amp;json_value)&#10;            .map_err(|e| SerializationError::SerializationFailed(e.to_string()))?;&#10;&#10;        // For a generic implementation, we'll store JSON as bytes&#10;        // Real implementation would use proper protobuf message definitions&#10;        buf.extend_from_slice(&amp;json_bytes);&#10;        Ok(buf)&#10;    }&#10;&#10;    fn deserialize_record(&#10;        &amp;self,&#10;        bytes: &amp;[u8],&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        // For generic protobuf support, decode from JSON bytes&#10;        let json_value: serde_json::Value = serde_json::from_slice(bytes)&#10;            .map_err(|e| SerializationError::DeserializationFailed(e.to_string()))?;&#10;&#10;        match json_value {&#10;            serde_json::Value::Object(map) =&gt; {&#10;                let mut record = HashMap::new();&#10;                for (key, value) in map {&#10;                    let field_value = json_to_field_value(&amp;value)?;&#10;                    record.insert(key, field_value);&#10;                }&#10;                Ok(record)&#10;            }&#10;            _ =&gt; Err(SerializationError::DeserializationFailed(&#10;                &quot;Expected JSON object from protobuf data&quot;.to_string(),&#10;            )),&#10;        }&#10;    }&#10;&#10;    fn to_execution_format(&#10;        &amp;self,&#10;        record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, InternalValue&gt;, SerializationError&gt; {&#10;        let mut execution_map = HashMap::new();&#10;&#10;        for (key, field_value) in record {&#10;            let internal_value = field_value_to_internal(field_value)?;&#10;            execution_map.insert(key.clone(), internal_value);&#10;        }&#10;&#10;        Ok(execution_map)&#10;    }&#10;&#10;    fn from_execution_format(&#10;        &amp;self,&#10;        data: &amp;HashMap&lt;String, InternalValue&gt;,&#10;    ) -&gt; Result&lt;HashMap&lt;String, FieldValue&gt;, SerializationError&gt; {&#10;        let mut record = HashMap::new();&#10;&#10;        for (key, internal_value) in data {&#10;            let field_value = internal_to_field_value(internal_value)?;&#10;            record.insert(key.clone(), field_value);&#10;        }&#10;&#10;        Ok(record)&#10;    }&#10;&#10;    fn format_name(&amp;self) -&gt; &amp;'static str {&#10;        &quot;Protobuf&quot;&#10;    }&#10;}&#10;&#10;// Helper for protobuf conversion&#10;#[cfg(feature = &quot;protobuf&quot;)]&#10;fn record_to_json_map(&#10;    record: &amp;HashMap&lt;String, FieldValue&gt;,&#10;) -&gt; Result&lt;serde_json::Map&lt;String, serde_json::Value&gt;, SerializationError&gt; {&#10;    let mut json_map = serde_json::Map::new();&#10;&#10;    for (key, field_value) in record {&#10;        let json_value = field_value_to_json(field_value)?;&#10;        json_map.insert(key.clone(), json_value);&#10;    }&#10;&#10;    Ok(json_map)&#10;}&#10;&#10;/// Factory for creating serialization formats&#10;///&#10;/// This factory provides a centralized way to create serialization format instances&#10;/// by name or with custom configuration. It handles feature detection and returns&#10;/// appropriate errors for unsupported formats.&#10;///&#10;/// # Examples&#10;///&#10;/// ```no_run&#10;/// use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;///&#10;/// // Create JSON format (always available)&#10;/// let json = SerializationFormatFactory::create_format(&quot;json&quot;).unwrap();&#10;///&#10;/// // Get list of supported formats&#10;/// let formats = SerializationFormatFactory::supported_formats();&#10;/// println!(&quot;Available: {:?}&quot;, formats);&#10;///&#10;/// // Get default format&#10;/// let default = SerializationFormatFactory::default_format();&#10;/// ```&#10;pub struct SerializationFormatFactory;&#10;&#10;impl SerializationFormatFactory {&#10;    /// Create a serialization format by name&#10;    ///&#10;    /// Creates a format instance using the default configuration for the specified&#10;    /// format. For schema-based formats like Avro, this uses a generic schema.&#10;    ///&#10;    /// # Supported Format Names&#10;    ///&#10;    /// - `&quot;json&quot;` - Always available&#10;    /// - `&quot;avro&quot;` - Requires `avro` feature, uses default schema&#10;    /// - `&quot;protobuf&quot;` or `&quot;proto&quot;` - Requires `protobuf` feature&#10;    ///&#10;    /// # Arguments&#10;    ///&#10;    /// * `format_name` - Case-insensitive format name&#10;    ///&#10;    /// # Returns&#10;    ///&#10;    /// * `Ok(Box&lt;dyn SerializationFormat&gt;)` - Format instance&#10;    /// * `Err(SerializationError::UnsupportedType)` - If format is unknown or feature not enabled&#10;    ///&#10;    /// # Examples&#10;    ///&#10;    /// ```no_run&#10;    /// use ferrisstreams::ferris::serialization::SerializationFormatFactory;&#10;    ///&#10;    /// let json_format = SerializationFormatFactory::create_format(&quot;json&quot;)?;&#10;    /// assert_eq!(json_format.format_name(), &quot;JSON&quot;);&#10;    /// # Ok::&lt;(), Box&lt;dyn std::error::Error&gt;&gt;(())&#10;    /// ```&#10;    pub fn create_format(&#10;        format_name: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        match format_name.to_lowercase().as_str() {&#10;            &quot;json&quot; =&gt; Ok(Box::new(JsonFormat)),&#10;            #[cfg(feature = &quot;avro&quot;)]&#10;            &quot;avro&quot; =&gt; {&#10;                let avro_format = AvroFormat::default_format().map_err(|e| {&#10;                    SerializationError::FormatConversionFailed(format!(&#10;                        &quot;Failed to create Avro format: {}&quot;,&#10;                        e&#10;                    ))&#10;                })?;&#10;                Ok(Box::new(avro_format))&#10;            }&#10;            #[cfg(feature = &quot;protobuf&quot;)]&#10;            &quot;protobuf&quot; | &quot;proto&quot; =&gt; {&#10;                // Generic protobuf format - in practice you'd specify the message type&#10;                Ok(Box::new(ProtobufFormat::&lt;()&gt;::new()))&#10;            }&#10;            _ =&gt; Err(SerializationError::UnsupportedType(format!(&#10;                &quot;Unknown format: {}&quot;,&#10;                format_name&#10;            ))),&#10;        }&#10;    }&#10;&#10;    /// Create Avro format with custom schema&#10;    #[cfg(feature = &quot;avro&quot;)]&#10;    pub fn create_avro_format(&#10;        schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        let avro_format = AvroFormat::new(schema_json)?;&#10;        Ok(Box::new(avro_format))&#10;    }&#10;&#10;    /// Create Avro format with schema evolution support&#10;    #[cfg(feature = &quot;avro&quot;)]&#10;    pub fn create_avro_format_with_schemas(&#10;        writer_schema_json: &amp;str,&#10;        reader_schema_json: &amp;str,&#10;    ) -&gt; Result&lt;Box&lt;dyn SerializationFormat&gt;, SerializationError&gt; {&#10;        let avro_format = AvroFormat::with_schemas(writer_schema_json, reader_schema_json)?;&#10;        Ok(Box::new(avro_format))&#10;    }&#10;&#10;    /// Create Protobuf format with specific message type&#10;    #[cfg(feature = &quot;protobuf&quot;)]&#10;    pub fn create_protobuf_format&lt;T&gt;() -&gt; Box&lt;dyn SerializationFormat&gt;&#10;    where&#10;        T: prost::Message + Default + Clone + Send + Sync + 'static,&#10;    {&#10;        Box::new(ProtobufFormat::&lt;T&gt;::new())&#10;    }&#10;&#10;    /// Get list of supported formats&#10;    pub fn supported_formats() -&gt; Vec&lt;&amp;'static str&gt; {&#10;        let mut formats = vec![&quot;json&quot;];&#10;&#10;        #[cfg(feature = &quot;avro&quot;)]&#10;        formats.push(&quot;avro&quot;);&#10;&#10;        #[cfg(feature = &quot;protobuf&quot;)]&#10;        {&#10;            formats.push(&quot;protobuf&quot;);&#10;            formats.push(&quot;proto&quot;);&#10;        }&#10;&#10;        formats&#10;    }&#10;&#10;    /// Get default format (JSON)&#10;    pub fn default_format() -&gt; Box&lt;dyn SerializationFormat&gt; {&#10;        Box::new(JsonFormat)&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/sql/join_tests.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/sql/join_tests.rs" />
              <option name="originalContent" value="/*!&#10;# Tests for JOIN Operations&#10;&#10;Comprehensive test suite for all JOIN types (INNER, LEFT, RIGHT, FULL OUTER) and windowed JOINs in streaming SQL.&#10;*/&#10;&#10;use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use std::collections::HashMap;&#10;use tokio::sync::mpsc;&#10;use ferrisstreams::ferris::serialization::JsonFormat;&#10;&#10;fn create_test_record_for_join() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(100.0));&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;async fn execute_join_query(&#10;    query: &amp;str,&#10;) -&gt; Result&lt;Vec&lt;HashMap&lt;String, serde_json::Value&gt;&gt;, Box&lt;dyn std::error::Error&gt;&gt; {&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let serialization_format = std::sync::Arc::new(JsonFormat);&#10;    let mut engine = StreamExecutionEngine::new(tx, serialization_format.clone());&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    let parsed_query = parser.parse(query)?;&#10;    let record = create_test_record_with_join_fields();&#10;&#10;    // Convert StreamRecord to HashMap&lt;String, serde_json::Value&gt;&#10;    let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;        .fields&#10;        .into_iter()&#10;        .map(|(k, v)| {&#10;            let json_val = match v {&#10;                FieldValue::Integer(i) =&gt; serde_json::Value::Number(serde_json::Number::from(i)),&#10;                FieldValue::Float(f) =&gt; {&#10;                    serde_json::Value::Number(serde_json::Number::from_f64(f).unwrap_or(0.into()))&#10;                }&#10;                FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                FieldValue::Null =&gt; serde_json::Value::Null,&#10;                _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;            };&#10;            (k, json_val)&#10;        })&#10;        .collect();&#10;&#10;    engine.execute(&amp;parsed_query, json_record).await?;&#10;&#10;    let mut results = Vec::new();&#10;    while let Ok(result) = rx.try_recv() {&#10;        results.push(result);&#10;    }&#10;    Ok(results)&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_basic_inner_join() {&#10;    // Test basic INNER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    // This should work with our mock implementation&#10;    let result = execute_join_query(query).await;&#10;&#10;    // For now, expect an error since we haven't implemented the parser yet&#10;    // Once the parser supports JOIN, this should succeed&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_alias() {&#10;    // Test JOIN with table aliases&#10;    let query = &quot;SELECT l.name, r.right_name FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_where_clause() {&#10;    // Test JOIN combined with WHERE clause&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id WHERE l.amount &gt; 50&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_field_access() {&#10;    // Test accessing joined fields&#10;    let query = &quot;SELECT id, name, right_name, right_value FROM left_stream INNER JOIN right_stream ON id = right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_multiple_joins() {&#10;    // Test multiple JOIN clauses (will be supported when parser is extended)&#10;    let query = &quot;SELECT * FROM stream1 s1 INNER JOIN stream2 s2 ON s1.id = s2.id INNER JOIN stream3 s3 ON s2.id = s3.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;    // Now supports multiple JOINs&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_outer_join() {&#10;    // Test LEFT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream LEFT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    // Should succeed now that parser supports LEFT JOIN&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_join_short_syntax() {&#10;    // Test LEFT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream LEFT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_outer_join() {&#10;    // Test RIGHT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream RIGHT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_join_short_syntax() {&#10;    // Test RIGHT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream RIGHT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_full_outer_join() {&#10;    // Test FULL OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream FULL OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join() {&#10;    // Test JOIN with WITHIN clause for temporal joins&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id WITHIN INTERVAL '5' MINUTES&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_seconds() {&#10;    // Test JOIN with WITHIN clause using seconds&#10;    let query = &quot;SELECT * FROM orders INNER JOIN payments p ON orders.id = p.order_id WITHIN INTERVAL '30' SECONDS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_hours() {&#10;    // Test JOIN with WITHIN clause using hours&#10;    let query = &quot;SELECT * FROM sessions LEFT JOIN events e ON sessions.user_id = e.user_id WITHIN INTERVAL '2' HOURS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_complex_condition() {&#10;    // Test JOIN with complex ON condition&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id AND l.amount &gt; 100&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_specific_fields() {&#10;    // Test JOIN with specific field selection - simplified to avoid alias issues for now&#10;    let query =&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream r ON left_stream.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_parsing_validation() {&#10;    // Test that invalid JOIN syntax is properly rejected&#10;    let invalid_queries = vec![&#10;        &quot;SELECT * FROM left_stream JOIN&quot;,              // Missing right side&#10;        &quot;SELECT * FROM left_stream JOIN right_stream&quot;, // Missing ON clause&#10;        &quot;SELECT * FROM left_stream INNER&quot;,             // Incomplete JOIN&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream ON&quot;, // Missing condition&#10;        &quot;SELECT * FROM left_stream FULL JOIN right_stream ON l.id = r.id&quot;, // FULL without OUTER&#10;    ];&#10;&#10;    for query in invalid_queries {&#10;        let result = execute_join_query(query).await;&#10;        assert!(result.is_err(), &quot;Query should have failed: {}&quot;, query);&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_stream_table_join_syntax() {&#10;    // Test stream-table JOIN which should be optimized differently&#10;    let query = &quot;SELECT s.user_id, s.event_type, t.user_name FROM events s INNER JOIN user_table t ON s.user_id = t.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;fn create_test_record_with_join_fields() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;user_id&quot;.to_string(), FieldValue::Integer(100));&#10;    fields.insert(&quot;order_id&quot;.to_string(), FieldValue::Integer(500));&#10;    fields.insert(&#10;        &quot;name&quot;.to_string(),&#10;        FieldValue::String(&quot;Test User&quot;.to_string()),&#10;    );&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(250.0));&#10;    fields.insert(&#10;        &quot;event_type&quot;.to_string(),&#10;        FieldValue::String(&quot;click&quot;.to_string()),&#10;    );&#10;    fields.insert(&#10;        &quot;status&quot;.to_string(),&#10;        FieldValue::String(&quot;active&quot;.to_string()),&#10;    );&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_execution_logic() {&#10;    // Test that the JOIN execution logic actually works with the parser&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let mut engine = StreamExecutionEngine::new(tx);&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    // This should parse successfully and execute the JOIN logic&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    match parser.parse(query) {&#10;        Ok(parsed_query) =&gt; {&#10;            let record = create_test_record_with_join_fields();&#10;&#10;            // Convert to JSON format for execution&#10;            let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;                .fields&#10;                .into_iter()&#10;                .map(|(k, v)| {&#10;                    let json_val = match v {&#10;                        FieldValue::Integer(i) =&gt; {&#10;                            serde_json::Value::Number(serde_json::Number::from(i))&#10;                        }&#10;                        FieldValue::Float(f) =&gt; serde_json::Value::Number(&#10;                            serde_json::Number::from_f64(f).unwrap_or(0.into()),&#10;                        ),&#10;                        FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                        FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                        FieldValue::Null =&gt; serde_json::Value::Null,&#10;                        _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;                    };&#10;                    (k, json_val)&#10;                })&#10;                .collect();&#10;&#10;            // Execute the query - this tests the JOIN execution engine&#10;            let execution_result = engine.execute(&amp;parsed_query, json_record).await;&#10;&#10;            // Should either succeed or fail gracefully with proper error&#10;            assert!(&#10;                execution_result.is_ok()&#10;                    || execution_result.unwrap_err().to_string().contains(&quot;JOIN&quot;)&#10;            );&#10;        }&#10;        Err(e) =&gt; {&#10;            // Parser should succeed with the new JOIN parsing logic&#10;            panic!(&quot;Failed to parse JOIN query: {}&quot;, e);&#10;        }&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="/*!&#10;# Tests for JOIN Operations&#10;&#10;Comprehensive test suite for all JOIN types (INNER, LEFT, RIGHT, FULL OUTER) and windowed JOINs in streaming SQL.&#10;*/&#10;&#10;use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use std::collections::HashMap;&#10;use tokio::sync::mpsc;&#10;use ferrisstreams::ferris::serialization::JsonFormat;&#10;&#10;fn create_test_record_for_join() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;name&quot;.to_string(), FieldValue::String(&quot;Alice&quot;.to_string()));&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(100.0));&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;async fn execute_join_query(&#10;    query: &amp;str,&#10;) -&gt; Result&lt;Vec&lt;HashMap&lt;String, serde_json::Value&gt;&gt;, Box&lt;dyn std::error::Error&gt;&gt; {&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let serialization_format = std::sync::Arc::new(JsonFormat);&#10;    let mut engine = StreamExecutionEngine::new(tx, serialization_format.clone());&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    let parsed_query = parser.parse(query)?;&#10;    let record = create_test_record_with_join_fields();&#10;&#10;    // Convert StreamRecord to HashMap&lt;String, serde_json::Value&gt;&#10;    let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;        .fields&#10;        .into_iter()&#10;        .map(|(k, v)| {&#10;            let json_val = match v {&#10;                FieldValue::Integer(i) =&gt; serde_json::Value::Number(serde_json::Number::from(i)),&#10;                FieldValue::Float(f) =&gt; {&#10;                    serde_json::Value::Number(serde_json::Number::from_f64(f).unwrap_or(0.into()))&#10;                }&#10;                FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                FieldValue::Null =&gt; serde_json::Value::Null,&#10;                _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;            };&#10;            (k, json_val)&#10;        })&#10;        .collect();&#10;&#10;    engine.execute(&amp;parsed_query, json_record).await?;&#10;&#10;    let mut results = Vec::new();&#10;    while let Ok(result) = rx.try_recv() {&#10;        results.push(result);&#10;    }&#10;    Ok(results)&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_basic_inner_join() {&#10;    // Test basic INNER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    // This should work with our mock implementation&#10;    let result = execute_join_query(query).await;&#10;&#10;    // For now, expect an error since we haven't implemented the parser yet&#10;    // Once the parser supports JOIN, this should succeed&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_alias() {&#10;    // Test JOIN with table aliases&#10;    let query = &quot;SELECT l.name, r.right_name FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_where_clause() {&#10;    // Test JOIN combined with WHERE clause&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id WHERE l.amount &gt; 50&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_field_access() {&#10;    // Test accessing joined fields&#10;    let query = &quot;SELECT id, name, right_name, right_value FROM left_stream INNER JOIN right_stream ON id = right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_err() || result.unwrap().len() &gt; 0);&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_multiple_joins() {&#10;    // Test multiple JOIN clauses (will be supported when parser is extended)&#10;    let query = &quot;SELECT * FROM stream1 s1 INNER JOIN stream2 s2 ON s1.id = s2.id INNER JOIN stream3 s3 ON s2.id = s3.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;    // Now supports multiple JOINs&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_outer_join() {&#10;    // Test LEFT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream LEFT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    // Should succeed now that parser supports LEFT JOIN&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_left_join_short_syntax() {&#10;    // Test LEFT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream LEFT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_outer_join() {&#10;    // Test RIGHT OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream RIGHT OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_right_join_short_syntax() {&#10;    // Test RIGHT JOIN (without OUTER keyword)&#10;    let query = &quot;SELECT * FROM left_stream RIGHT JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_full_outer_join() {&#10;    // Test FULL OUTER JOIN syntax&#10;    let query = &quot;SELECT * FROM left_stream FULL OUTER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join() {&#10;    // Test JOIN with WITHIN clause for temporal joins&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id WITHIN INTERVAL '5' MINUTES&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_seconds() {&#10;    // Test JOIN with WITHIN clause using seconds&#10;    let query = &quot;SELECT * FROM orders INNER JOIN payments p ON orders.id = p.order_id WITHIN INTERVAL '30' SECONDS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_windowed_join_hours() {&#10;    // Test JOIN with WITHIN clause using hours&#10;    let query = &quot;SELECT * FROM sessions LEFT JOIN events e ON sessions.user_id = e.user_id WITHIN INTERVAL '2' HOURS&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_complex_condition() {&#10;    // Test JOIN with complex ON condition&#10;    let query = &quot;SELECT * FROM left_stream l INNER JOIN right_stream r ON l.id = r.right_id AND l.amount &gt; 100&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_with_specific_fields() {&#10;    // Test JOIN with specific field selection - simplified to avoid alias issues for now&#10;    let query =&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream r ON left_stream.id = r.right_id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    match &amp;result {&#10;        Ok(res) =&gt; println!(&quot;SUCCESS: Got {} results&quot;, res.len()),&#10;        Err(e) =&gt; println!(&quot;ERROR: {}&quot;, e),&#10;    }&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_parsing_validation() {&#10;    // Test that invalid JOIN syntax is properly rejected&#10;    let invalid_queries = vec![&#10;        &quot;SELECT * FROM left_stream JOIN&quot;,              // Missing right side&#10;        &quot;SELECT * FROM left_stream JOIN right_stream&quot;, // Missing ON clause&#10;        &quot;SELECT * FROM left_stream INNER&quot;,             // Incomplete JOIN&#10;        &quot;SELECT * FROM left_stream INNER JOIN right_stream ON&quot;, // Missing condition&#10;        &quot;SELECT * FROM left_stream FULL JOIN right_stream ON l.id = r.id&quot;, // FULL without OUTER&#10;    ];&#10;&#10;    for query in invalid_queries {&#10;        let result = execute_join_query(query).await;&#10;        assert!(result.is_err(), &quot;Query should have failed: {}&quot;, query);&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_stream_table_join_syntax() {&#10;    // Test stream-table JOIN which should be optimized differently&#10;    let query = &quot;SELECT s.user_id, s.event_type, t.user_name FROM events s INNER JOIN user_table t ON s.user_id = t.id&quot;;&#10;&#10;    let result = execute_join_query(query).await;&#10;    assert!(result.is_ok() || result.unwrap_err().to_string().contains(&quot;JOIN&quot;));&#10;}&#10;&#10;fn create_test_record_with_join_fields() -&gt; StreamRecord {&#10;    let mut fields = HashMap::new();&#10;    fields.insert(&quot;id&quot;.to_string(), FieldValue::Integer(1));&#10;    fields.insert(&quot;user_id&quot;.to_string(), FieldValue::Integer(100));&#10;    fields.insert(&quot;order_id&quot;.to_string(), FieldValue::Integer(500));&#10;    fields.insert(&#10;        &quot;name&quot;.to_string(),&#10;        FieldValue::String(&quot;Test User&quot;.to_string()),&#10;    );&#10;    fields.insert(&quot;amount&quot;.to_string(), FieldValue::Float(250.0));&#10;    fields.insert(&#10;        &quot;event_type&quot;.to_string(),&#10;        FieldValue::String(&quot;click&quot;.to_string()),&#10;    );&#10;    fields.insert(&#10;        &quot;status&quot;.to_string(),&#10;        FieldValue::String(&quot;active&quot;.to_string()),&#10;    );&#10;&#10;    StreamRecord {&#10;        fields,&#10;        headers: HashMap::new(),&#10;        timestamp: 1234567890000,&#10;        offset: 1,&#10;        partition: 0,&#10;    }&#10;}&#10;&#10;#[tokio::test]&#10;async fn test_join_execution_logic() {&#10;    // Test that the JOIN execution logic actually works with the parser&#10;    let (tx, mut rx) = mpsc::unbounded_channel();&#10;    let mut engine = StreamExecutionEngine::new(tx);&#10;    let parser = StreamingSqlParser::new();&#10;&#10;    // This should parse successfully and execute the JOIN logic&#10;    let query = &quot;SELECT * FROM left_stream INNER JOIN right_stream ON left_stream.id = right_stream.right_id&quot;;&#10;&#10;    match parser.parse(query) {&#10;        Ok(parsed_query) =&gt; {&#10;            let record = create_test_record_with_join_fields();&#10;&#10;            // Convert to JSON format for execution&#10;            let json_record: HashMap&lt;String, serde_json::Value&gt; = record&#10;                .fields&#10;                .into_iter()&#10;                .map(|(k, v)| {&#10;                    let json_val = match v {&#10;                        FieldValue::Integer(i) =&gt; {&#10;                            serde_json::Value::Number(serde_json::Number::from(i))&#10;                        }&#10;                        FieldValue::Float(f) =&gt; serde_json::Value::Number(&#10;                            serde_json::Number::from_f64(f).unwrap_or(0.into()),&#10;                        ),&#10;                        FieldValue::String(s) =&gt; serde_json::Value::String(s),&#10;                        FieldValue::Boolean(b) =&gt; serde_json::Value::Bool(b),&#10;                        FieldValue::Null =&gt; serde_json::Value::Null,&#10;                        _ =&gt; serde_json::Value::String(format!(&quot;{:?}&quot;, v)),&#10;                    };&#10;                    (k, json_val)&#10;                })&#10;                .collect();&#10;&#10;            // Execute the query - this tests the JOIN execution engine&#10;            let execution_result = engine.execute(&amp;parsed_query, json_record).await;&#10;&#10;            // Should either succeed or fail gracefully with proper error&#10;            assert!(&#10;                execution_result.is_ok()&#10;                    || execution_result.unwrap_err().to_string().contains(&quot;JOIN&quot;)&#10;            );&#10;        }&#10;        Err(e) =&gt; {&#10;            // Parser should succeed with the new JOIN parsing logic&#10;            panic!(&quot;Failed to parse JOIN query: {}&quot;, e);&#10;        }&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/sql/limit_tests.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/sql/limit_tests.rs" />
              <option name="originalContent" value="use ferrisstreams::ferris::sql::execution::StreamExecutionEngine;&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use ferrisstreams::ferris::serialization::{JsonFormat, InternalValue};&#10;use std::collections::HashMap;&#10;use std::sync::Arc;&#10;use tokio::sync::mpsc;&#10;&#10;#[cfg(test)]&#10;mod tests {&#10;    use super::*;&#10;&#10;    #[test]&#10;    fn test_limit_parsing() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test parsing queries with LIMIT&#10;        let queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT 10&quot;,&#10;            &quot;SELECT customer_id, amount FROM orders LIMIT 5&quot;,&#10;            &quot;SELECT * FROM orders WHERE amount &gt; 100 LIMIT 3&quot;,&#10;        ];&#10;&#10;        for query in queries {&#10;            let result = parser.parse(query);&#10;            assert!(result.is_ok(), &quot;Failed to parse query: {}&quot;, query);&#10;&#10;            // Verify limit is parsed correctly&#10;            if let Ok(parsed_query) = result {&#10;                match parsed_query {&#10;                    ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                        assert!(&#10;                            limit.is_some(),&#10;                            &quot;LIMIT should be parsed for query: {}&quot;,&#10;                            query&#10;                        );&#10;                    }&#10;                    _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;                }&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_values() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        let test_cases = vec![&#10;            (&quot;SELECT * FROM orders LIMIT 1&quot;, 1),&#10;            (&quot;SELECT * FROM orders LIMIT 10&quot;, 10),&#10;            (&quot;SELECT * FROM orders LIMIT 100&quot;, 100),&#10;            (&quot;SELECT * FROM orders LIMIT 1000&quot;, 1000),&#10;        ];&#10;&#10;        for (query, expected_limit) in test_cases {&#10;            let result = parser.parse(query).unwrap();&#10;            match result {&#10;                ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                    assert_eq!(&#10;                        limit,&#10;                        Some(expected_limit),&#10;                        &quot;Limit value mismatch for query: {}&quot;,&#10;                        query&#10;                    );&#10;                }&#10;                _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;            }&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_execution_basic() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 2&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders LIMIT 2&quot;)&#10;            .unwrap();&#10;&#10;        // Create test records&#10;        let mut records: Vec&lt;HashMap&lt;String, InternalValue&gt;&gt; = Vec::new();&#10;        for i in 1..=3 {&#10;            let mut record = HashMap::new();&#10;            record.insert(&#10;                &quot;customer_id&quot;.to_string(),&#10;                InternalValue::Integer(i),&#10;            );&#10;            record.insert(&#10;                &quot;amount&quot;.to_string(),&#10;                InternalValue::Number(100.0 * i as f64),&#10;            );&#10;            // Execute each record individually&#10;            engine.execute(&amp;query, record).await.unwrap();&#10;        }&#10;&#10;        // Check results&#10;        let mut count = 0;&#10;        while let Some(_result) = rx.recv().await {&#10;            count += 1;&#10;            assert!(count &lt;= 2, &quot;Should not receive more than 2 records due to LIMIT&quot;);&#10;        }&#10;        assert_eq!(count, 2, &quot;Should receive exactly 2 records due to LIMIT&quot;);&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_where_clause() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with WHERE and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders WHERE amount &gt; 150 LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(3));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(300.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output (first record matching WHERE clause)&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;amount&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::Number(amount)) =&gt; {&#10;                    assert_eq!(*id, 2);&#10;                    assert_eq!(*amount, 200.0);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second matching record due to LIMIT&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_zero() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 0&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id FROM orders LIMIT 0&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&#10;            &quot;customer_id&quot;.to_string(),&#10;            Value::Number(serde_json::Number::from(1)),&#10;        );&#10;&#10;        // Execute record&#10;        let result = engine.execute(&amp;query, record).await;&#10;        assert!(result.is_ok());&#10;&#10;        // Should not receive any output due to LIMIT 0&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive any records due to LIMIT 0&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_csas() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse CSAS query with LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;CREATE STREAM limited_orders AS SELECT customer_id, amount FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;        } else {&#10;            panic!(&quot;Expected to receive one record&quot;);&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT in CSAS&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_system_columns() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with system columns and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, _timestamp, _partition FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;            assert!(output.contains_key(&quot;_timestamp&quot;));&#10;            assert!(output.contains_key(&quot;_partition&quot;));&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_headers() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Create test headers&#10;        let mut headers = HashMap::new();&#10;        headers.insert(&quot;source&quot;.to_string(), &quot;test-app&quot;.to_string());&#10;&#10;        // Parse query with header function and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, HEADER('source') AS source FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute_with_headers(&amp;query, record, headers.clone()).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute_with_headers(&amp;query, record, headers).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;source&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::String(source)) =&gt; {&#10;                    assert_eq!(*id, 1);&#10;                    assert_eq!(source, &quot;test-app&quot;);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_errors() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test invalid LIMIT values&#10;        let invalid_queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT -1&quot;,  // Negative not handled gracefully&#10;            &quot;SELECT * FROM orders LIMIT abc&quot;, // Non-numeric&#10;            &quot;SELECT * FROM orders LIMIT 1.5&quot;, // Float&#10;        ];&#10;&#10;        for query in invalid_queries {&#10;            let result = parser.parse(query);&#10;            // These should fail at parse time or be handled gracefully&#10;            if result.is_ok() {&#10;                println!(&quot;Query parsed unexpectedly: {}&quot;, query);&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_query_without_limit() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test query without LIMIT clause&#10;        let query = &quot;SELECT * FROM orders WHERE amount &gt; 100&quot;;&#10;        let result = parser.parse(query).unwrap();&#10;&#10;        match result {&#10;            ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                assert_eq!(limit, None, &quot;Query without LIMIT should have None limit&quot;);&#10;            }&#10;            _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;        }&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="use ferrisstreams::ferris::sql::execution::StreamExecutionEngine;&#10;use ferrisstreams::ferris::sql::parser::StreamingSqlParser;&#10;use ferrisstreams::ferris::serialization::{JsonFormat, InternalValue};&#10;use std::collections::HashMap;&#10;use std::sync::Arc;&#10;use tokio::sync::mpsc;&#10;&#10;#[cfg(test)]&#10;mod tests {&#10;    use super::*;&#10;&#10;    #[test]&#10;    fn test_limit_parsing() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test parsing queries with LIMIT&#10;        let queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT 10&quot;,&#10;            &quot;SELECT customer_id, amount FROM orders LIMIT 5&quot;,&#10;            &quot;SELECT * FROM orders WHERE amount &gt; 100 LIMIT 3&quot;,&#10;        ];&#10;&#10;        for query in queries {&#10;            let result = parser.parse(query);&#10;            assert!(result.is_ok(), &quot;Failed to parse query: {}&quot;, query);&#10;&#10;            // Verify limit is parsed correctly&#10;            if let Ok(parsed_query) = result {&#10;                match parsed_query {&#10;                    ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                        assert!(&#10;                            limit.is_some(),&#10;                            &quot;LIMIT should be parsed for query: {}&quot;,&#10;                            query&#10;                        );&#10;                    }&#10;                    _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;                }&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_values() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        let test_cases = vec![&#10;            (&quot;SELECT * FROM orders LIMIT 1&quot;, 1),&#10;            (&quot;SELECT * FROM orders LIMIT 10&quot;, 10),&#10;            (&quot;SELECT * FROM orders LIMIT 100&quot;, 100),&#10;            (&quot;SELECT * FROM orders LIMIT 1000&quot;, 1000),&#10;        ];&#10;&#10;        for (query, expected_limit) in test_cases {&#10;            let result = parser.parse(query).unwrap();&#10;            match result {&#10;                ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                    assert_eq!(&#10;                        limit,&#10;                        Some(expected_limit),&#10;                        &quot;Limit value mismatch for query: {}&quot;,&#10;                        query&#10;                    );&#10;                }&#10;                _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;            }&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_execution_basic() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 2&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders LIMIT 2&quot;)&#10;            .unwrap();&#10;&#10;        // Create test records&#10;        let mut records: Vec&lt;HashMap&lt;String, InternalValue&gt;&gt; = Vec::new();&#10;        for i in 1..=3 {&#10;            let mut record = HashMap::new();&#10;            record.insert(&#10;                &quot;customer_id&quot;.to_string(),&#10;                InternalValue::Integer(i),&#10;            );&#10;            record.insert(&#10;                &quot;amount&quot;.to_string(),&#10;                InternalValue::Number(100.0 * i as f64),&#10;            );&#10;            // Execute each record individually&#10;            engine.execute(&amp;query, record).await.unwrap();&#10;        }&#10;&#10;        // Check results&#10;        let mut count = 0;&#10;        while let Some(_result) = rx.recv().await {&#10;            count += 1;&#10;            assert!(count &lt;= 2, &quot;Should not receive more than 2 records due to LIMIT&quot;);&#10;        }&#10;        assert_eq!(count, 2, &quot;Should receive exactly 2 records due to LIMIT&quot;);&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_where_clause() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with WHERE and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, amount FROM orders WHERE amount &gt; 150 LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(3));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(300.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output (first record matching WHERE clause)&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;amount&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::Number(amount)) =&gt; {&#10;                    assert_eq!(*id, 2);&#10;                    assert_eq!(*amount, 200.0);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second matching record due to LIMIT&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_zero() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with LIMIT 0&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id FROM orders LIMIT 0&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should not receive any output due to LIMIT 0&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive any records due to LIMIT 0&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_csas() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse CSAS query with LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;CREATE STREAM limited_orders AS SELECT customer_id, amount FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(100.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        record.insert(&quot;amount&quot;.to_string(), InternalValue::Number(200.0));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;        } else {&#10;            panic!(&quot;Expected to receive one record&quot;);&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT in CSAS&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_system_columns() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Parse query with system columns and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, _timestamp, _partition FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute(&amp;query, record).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match &amp;output[&quot;customer_id&quot;] {&#10;                InternalValue::Integer(id) =&gt; assert_eq!(*id, 1),&#10;                _ =&gt; panic!(&quot;Unexpected customer_id type&quot;),&#10;            }&#10;            assert!(output.contains_key(&quot;_timestamp&quot;));&#10;            assert!(output.contains_key(&quot;_partition&quot;));&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_limit_with_headers() {&#10;        // Setup execution engine&#10;        let (tx, mut rx) = mpsc::unbounded_channel();&#10;        let mut engine = StreamExecutionEngine::new(tx, Arc::new(JsonFormat));&#10;&#10;        // Create test headers&#10;        let mut headers = HashMap::new();&#10;        headers.insert(&quot;source&quot;.to_string(), &quot;test-app&quot;.to_string());&#10;&#10;        // Parse query with header function and LIMIT&#10;        let parser = StreamingSqlParser::new();&#10;        let query = parser&#10;            .parse(&quot;SELECT customer_id, HEADER('source') AS source FROM orders LIMIT 1&quot;)&#10;            .unwrap();&#10;&#10;        // Create and execute test records&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(1));&#10;        engine.execute_with_headers(&amp;query, record, headers.clone()).await.unwrap();&#10;&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;customer_id&quot;.to_string(), InternalValue::Integer(2));&#10;        engine.execute_with_headers(&amp;query, record, headers).await.unwrap();&#10;&#10;        // Should receive exactly 1 output&#10;        if let Some(output) = rx.recv().await {&#10;            match (&amp;output[&quot;customer_id&quot;], &amp;output[&quot;source&quot;]) {&#10;                (InternalValue::Integer(id), InternalValue::String(source)) =&gt; {&#10;                    assert_eq!(*id, 1);&#10;                    assert_eq!(source, &quot;test-app&quot;);&#10;                },&#10;                _ =&gt; panic!(&quot;Unexpected output types&quot;),&#10;            }&#10;        }&#10;&#10;        // Should not receive second record&#10;        assert!(&#10;            rx.try_recv().is_err(),&#10;            &quot;Should not receive second record due to LIMIT&quot;&#10;        );&#10;    }&#10;&#10;    #[test]&#10;    fn test_limit_parsing_errors() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test invalid LIMIT values&#10;        let invalid_queries = vec![&#10;            &quot;SELECT * FROM orders LIMIT -1&quot;,  // Negative not handled gracefully&#10;            &quot;SELECT * FROM orders LIMIT abc&quot;, // Non-numeric&#10;            &quot;SELECT * FROM orders LIMIT 1.5&quot;, // Float&#10;        ];&#10;&#10;        for query in invalid_queries {&#10;            let result = parser.parse(query);&#10;            // These should fail at parse time or be handled gracefully&#10;            if result.is_ok() {&#10;                println!(&quot;Query parsed unexpectedly: {}&quot;, query);&#10;            }&#10;        }&#10;    }&#10;&#10;    #[test]&#10;    fn test_query_without_limit() {&#10;        let parser = StreamingSqlParser::new();&#10;&#10;        // Test query without LIMIT clause&#10;        let query = &quot;SELECT * FROM orders WHERE amount &gt; 100&quot;;&#10;        let result = parser.parse(query).unwrap();&#10;&#10;        match result {&#10;            ferrisstreams::ferris::sql::ast::StreamingQuery::Select { limit, .. } =&gt; {&#10;                assert_eq!(limit, None, &quot;Query without LIMIT should have None limit&quot;);&#10;            }&#10;            _ =&gt; panic!(&quot;Expected SELECT query&quot;),&#10;        }&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/unit/sql/execution_test.rs">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/unit/sql/execution_test.rs" />
              <option name="originalContent" value="#[cfg(test)]&#10;mod tests {&#10;    use ferrisstreams::ferris::sql::ast::{BinaryOperator, Expr, LiteralValue};&#10;    use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;    use ferrisstreams::ferris::serialization::{InternalValue, JsonFormat};&#10;    use std::collections::HashMap;&#10;    use std::sync::Arc;&#10;    use tokio::sync::mpsc;&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        let record = StreamRecord {&#10;            fields: record.into_iter()&#10;                .map(|(k, v)| (k, FieldValue::String(v)))&#10;                .collect(),&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Test cases for LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, true),      // Matches prefix&#10;            (&quot;%World&quot;, true),      // Matches suffix&#10;            (&quot;%llo%&quot;, true),       // Matches substring&#10;            (&quot;Hello_World&quot;, true), // Matches exact with underscore&#10;            (&quot;hello%&quot;, false),     // Case sensitive no match&#10;            (&quot;Goodbye%&quot;, false),   // No match&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::Like,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed&quot;);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_not_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        let record = StreamRecord {&#10;            fields: record.into_iter()&#10;                .map(|(k, v)| (k, FieldValue::String(v)))&#10;                .collect(),&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Test cases for NOT LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, false),     // Matches prefix (so NOT LIKE is false)&#10;            (&quot;%World&quot;, false),     // Matches suffix (so NOT LIKE is false)&#10;            (&quot;%llo%&quot;, false),      // Matches substring (so NOT LIKE is false)&#10;            (&quot;Hello_World&quot;, false),// Matches exact with underscore (so NOT LIKE is false)&#10;            (&quot;hello%&quot;, true),      // Case sensitive no match (so NOT LIKE is true)&#10;            (&quot;Goodbye%&quot;, true),    // No match (so NOT LIKE is true)&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::NotLike,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;NOT LIKE operator evaluation failed&quot;);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator_edge_cases() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record with various types&#10;        let mut fields = HashMap::new();&#10;        fields.insert(&quot;text_field&quot;.to_string(), FieldValue::String(&quot;Hello World&quot;.to_string()));&#10;        fields.insert(&quot;null_field&quot;.to_string(), FieldValue::Null);&#10;        fields.insert(&quot;number_field&quot;.to_string(), FieldValue::Integer(123));&#10;&#10;        let record = StreamRecord {&#10;            fields,&#10;            timestamp: 0,&#10;            offset: 0,&#10;            partition: 0,&#10;            headers: HashMap::new(),&#10;        };&#10;&#10;        // Edge cases&#10;        let test_cases = vec![&#10;            // Empty pattern&#10;            (&quot;&quot;, true),&#10;            // Just wildcards&#10;            (&quot;%&quot;, true),&#10;            (&quot;%%&quot;, true),&#10;            (&quot;___&quot;, false),  // Three underscores won't match &quot;Hello World&quot;&#10;            // Escape characters (if implemented)&#10;            (&quot;Hello\\_World&quot;, true),&#10;            // Complex patterns&#10;            (&quot;H%W%d&quot;, true),&#10;            (&quot;%H%W%d%&quot;, true),&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let expr = Expr::BinaryOp {&#10;                left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                op: BinaryOperator::Like,&#10;                right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;            };&#10;&#10;            let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed for pattern: {}&quot;, pattern);&#10;            assert_eq!(result.unwrap(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;&#10;        // Test NULL field&#10;        let expr = Expr::BinaryOp {&#10;            left: Box::new(Expr::Column(&quot;null_field&quot;.to_string())),&#10;            op: BinaryOperator::Like,&#10;            right: Box::new(Expr::Literal(LiteralValue::String(&quot;%&quot;.to_string()))),&#10;        };&#10;        let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;        assert!(result.is_ok());&#10;        assert_eq!(result.unwrap(), false);  // NULL LIKE anything should be false&#10;&#10;        // Test numeric field with LIKE&#10;        let expr = Expr::BinaryOp {&#10;            left: Box::new(Expr::Column(&quot;number_field&quot;.to_string())),&#10;            op: BinaryOperator::Like,&#10;            right: Box::new(Expr::Literal(LiteralValue::String(&quot;%3&quot;.to_string()))),&#10;        };&#10;        let result = engine.evaluate_expression(&amp;expr, &amp;record).await;&#10;        assert!(result.is_ok());&#10;        assert_eq!(result.unwrap(), true);  // &quot;123&quot; LIKE &quot;%3&quot; should be true&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="#[cfg(test)]&#10;mod tests {&#10;    use ferrisstreams::ferris::sql::ast::{BinaryOperator, Expr, LiteralValue, SelectField, StreamSource, StreamingQuery};&#10;    use ferrisstreams::ferris::sql::execution::{FieldValue, StreamExecutionEngine, StreamRecord};&#10;    use ferrisstreams::ferris::serialization::JsonFormat;&#10;    use std::collections::HashMap;&#10;    use std::sync::Arc;&#10;    use tokio::sync::mpsc;&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator() {&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        &#10;        // Test cases for LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, true),      // Matches prefix&#10;            (&quot;%World&quot;, true),      // Matches suffix&#10;            (&quot;%llo%&quot;, true),       // Matches substring&#10;            (&quot;Hello_World&quot;, true), // Matches exact with underscore&#10;            (&quot;hello%&quot;, false),     // Case sensitive no match&#10;            (&quot;Goodbye%&quot;, false),   // No match&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::Like,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed&quot;);&#10;            let output = result.unwrap();&#10;            // The output will be true/false based on whether any results were produced&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_not_like_operator() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;&#10;        // Test cases for NOT LIKE operator&#10;        let test_cases = vec![&#10;            (&quot;Hello%&quot;, false),     // Matches prefix (so NOT LIKE is false)&#10;            (&quot;%World&quot;, false),     // Matches suffix (so NOT LIKE is false)&#10;            (&quot;%llo%&quot;, false),      // Matches substring (so NOT LIKE is false)&#10;            (&quot;Hello_World&quot;, false),// Matches exact with underscore (so NOT LIKE is false)&#10;            (&quot;hello%&quot;, true),      // Case sensitive no match (so NOT LIKE is true)&#10;            (&quot;Goodbye%&quot;, true),    // No match (so NOT LIKE is true)&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::NotLike,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;NOT LIKE operator evaluation failed&quot;);&#10;            let output = result.unwrap();&#10;            // The output will be true/false based on whether any results were produced&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;    }&#10;&#10;    #[tokio::test]&#10;    async fn test_like_operator_edge_cases() {&#10;        // Setup execution engine&#10;        let (tx, _rx) = mpsc::unbounded_channel();&#10;        let serialization_format = Arc::new(JsonFormat);&#10;        let mut engine = StreamExecutionEngine::new(tx, serialization_format);&#10;&#10;        // Create test record with various types&#10;        let mut record = HashMap::new();&#10;        record.insert(&quot;text_field&quot;.to_string(), &quot;Hello World&quot;.to_string());&#10;        record.insert(&quot;null_field&quot;.to_string(), &quot;null&quot;.to_string());  // Represents NULL&#10;        record.insert(&quot;number_field&quot;.to_string(), &quot;123&quot;.to_string());  // Number as string&#10;&#10;        // Edge cases&#10;        let test_cases = vec![&#10;            // Empty pattern&#10;            (&quot;&quot;, true),&#10;            // Just wildcards&#10;            (&quot;%&quot;, true),&#10;            (&quot;%%&quot;, true),&#10;            (&quot;___&quot;, false),  // Three underscores won't match &quot;Hello World&quot;&#10;            // Escape characters (if implemented)&#10;            (&quot;Hello\\_World&quot;, true),&#10;            // Complex patterns&#10;            (&quot;H%W%d&quot;, true),&#10;            (&quot;%H%W%d%&quot;, true),&#10;        ];&#10;&#10;        for (pattern, expected) in test_cases {&#10;            let query = StreamingQuery::Select {&#10;                fields: vec![SelectField::Expression {&#10;                    expr: Expr::BinaryOp {&#10;                        left: Box::new(Expr::Column(&quot;text_field&quot;.to_string())),&#10;                        op: BinaryOperator::Like,&#10;                        right: Box::new(Expr::Literal(LiteralValue::String(pattern.to_string()))),&#10;                    },&#10;                    alias: None,&#10;                }],&#10;                from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;                where_clause: None,&#10;                joins: None,&#10;                group_by: None,&#10;                having: None,&#10;                window: None,&#10;                limit: None,&#10;            };&#10;&#10;            let result = engine.execute(&amp;query, record.clone()).await;&#10;            assert!(result.is_ok(), &quot;LIKE operator evaluation failed for pattern: {}&quot;, pattern);&#10;            let output = result.unwrap();&#10;            assert_eq!(output.is_ok(), expected, &quot;Pattern '{}' failed&quot;, pattern);&#10;        }&#10;&#10;        // Test NULL field&#10;        let query = StreamingQuery::Select {&#10;            fields: vec![SelectField::Expression {&#10;                expr: Expr::BinaryOp {&#10;                    left: Box::new(Expr::Column(&quot;null_field&quot;.to_string())),&#10;                    op: BinaryOperator::Like,&#10;                    right: Box::new(Expr::Literal(LiteralValue::String(&quot;%&quot;.to_string()))),&#10;                },&#10;                alias: None,&#10;            }],&#10;            from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;            where_clause: None,&#10;            joins: None,&#10;            group_by: None,&#10;            having: None,&#10;            window: None,&#10;            limit: None,&#10;        };&#10;&#10;        let result = engine.execute(&amp;query, record.clone()).await;&#10;        assert!(result.is_ok());&#10;        let output = result.unwrap();&#10;        assert_eq!(output.is_ok(), false);  // NULL LIKE anything should be false&#10;&#10;        // Test numeric field with LIKE&#10;        let query = StreamingQuery::Select {&#10;            fields: vec![SelectField::Expression {&#10;                expr: Expr::BinaryOp {&#10;                    left: Box::new(Expr::Column(&quot;number_field&quot;.to_string())),&#10;                    op: BinaryOperator::Like,&#10;                    right: Box::new(Expr::Literal(LiteralValue::String(&quot;%3&quot;.to_string()))),&#10;                },&#10;                alias: None,&#10;            }],&#10;            from: StreamSource::Stream(&quot;test&quot;.to_string()),&#10;            where_clause: None,&#10;            joins: None,&#10;            group_by: None,&#10;            having: None,&#10;            window: None,&#10;            limit: None,&#10;        };&#10;&#10;        let result = engine.execute(&amp;query, record.clone()).await;&#10;        assert!(result.is_ok());&#10;        let output = result.unwrap();&#10;        assert_eq!(output.is_ok(), true);  // &quot;123&quot; LIKE &quot;%3&quot; should be true&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>