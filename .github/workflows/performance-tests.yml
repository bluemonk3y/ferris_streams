name: Performance Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

# Add permissions for the workflow to comment on PRs
permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    services:
      kafka:
        image: confluentinc/cp-kafka:7.9.1
        env:
          KAFKA_NODE_ID: 1
          KAFKA_PROCESS_ROLES: "broker,controller"
          KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://kafka:29093"
          KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://localhost:9092"
          KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"
          KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
          CLUSTER_ID: "citest123456789012345678901"
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
          KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
          KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
        ports:
          - 9092:9092
          - 29093:29093
        options: >-
          --health-cmd "kafka-topics --bootstrap-server localhost:9092 --list"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 10

    steps:
    - uses: actions/checkout@v4
    
    - name: Install Protocol Buffers compiler
      run: |
        sudo apt-get update
        sudo apt-get install -y protobuf-compiler

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Wait for Kafka (KRaft)
      run: |
        echo "Waiting for Kafka KRaft to be ready..."
        timeout 120 bash -c 'until nc -z localhost 9092; do sleep 2; done'
        echo "Kafka port is open, waiting for cluster to be ready..."
        sleep 15
        echo "Kafka KRaft is ready!"
    
    - name: Run JSON Performance Test
      run: |
        echo "Running JSON Performance Test..."
        cargo run --release --example json_performance_test > json_perf_results.txt 2>&1
        cat json_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Raw Bytes Performance Test
      run: |
        echo "Running Raw Bytes Performance Test..."
        cargo run --release --example raw_bytes_performance_test > raw_perf_results.txt 2>&1
        cat raw_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Latency Performance Test
      run: |
        echo "Running Latency Performance Test..."
        cargo run --release --example latency_performance_test > latency_perf_results.txt 2>&1
        cat latency_perf_results.txt
        echo "Waiting 45 seconds before next performance test..."
        sleep 45
    
    - name: Run Resource Monitoring Test
      run: |
        echo "Running Resource Monitoring Test..."
        cargo run --release --example resource_monitoring_test > resource_perf_results.txt 2>&1
        cat resource_perf_results.txt

    - name: Run FerrisStreams SQL Performance Benchmarks
      run: |
        echo "Running FerrisStreams SQL Performance Benchmarks..."
        echo "These benchmarks will run in CI mode with reduced dataset sizes"
        export CI=true
        export GITHUB_ACTIONS=true
        # Run ignored performance benchmarks with reduced scope
        echo "Running ferris_sql_multi_benchmarks..."
        timeout 300 cargo test --ignored --test ferris_sql_multi_benchmarks --no-default-features \
          benchmark_simple_select_baseline \
          benchmark_complex_aggregation \
          -- --nocapture > ferris_sql_benchmarks.txt 2>&1 || echo "ferris_sql_multi_benchmarks failed or timed out"
        
        echo "SQL benchmark step 1 complete, checking output..."
        if [ -f ferris_sql_benchmarks.txt ]; then
          echo "ferris_sql_benchmarks.txt exists, size: $(wc -l < ferris_sql_benchmarks.txt) lines"
        fi
        
        # Also run transactional processor benchmarks
        echo "Running transactional_processor_benchmarks..."
        timeout 300 cargo test --ignored --test transactional_processor_benchmarks --no-default-features \
          benchmark_simple_vs_transactional_small_batch \
          -- --nocapture >> ferris_sql_benchmarks.txt 2>&1 || echo "transactional_processor_benchmarks failed or timed out"
        echo "FerrisStreams SQL benchmarks completed"
        if [ -f ferris_sql_benchmarks.txt ]; then
          echo "=== FerrisStreams SQL Benchmark Results ==="
          cat ferris_sql_benchmarks.txt | grep -E "(BASELINE|AGGREGATION|PROCESSOR|records/sec|✅|❌|Config:)" || true
        fi
    
    - name: Extract Performance Metrics
      run: |
        echo "Extracting performance metrics..."
        
        # Debug: Show relevant lines from results files
        echo "=== JSON Performance Results (Messages Sent lines) ==="
        grep "Messages Sent" json_perf_results.txt || echo "No 'Messages Sent' lines found in json_perf_results.txt"
        
        echo "=== Raw Performance Results (Messages Sent lines) ==="
        grep "Messages Sent" raw_perf_results.txt || echo "No 'Messages Sent' lines found in raw_perf_results.txt"
        
        echo "=== Latency Performance Results (P95 Latency lines) ==="
        grep "P95 Latency" latency_perf_results.txt || echo "No 'P95 Latency' lines found in latency_perf_results.txt"
        
        # Extract JSON throughput (matches format: "Messages Sent:     12345 (123.4 msg/s)")
        JSON_THROUGHPUT=$(grep -oP 'Messages Sent:\s+\d+\s+\(\K\d+\.\d+(?=\s+msg/s\))' json_perf_results.txt | head -1 || echo "0")
        
        # Extract Raw throughput (matches format: "Messages Sent:     12345 (123.4 msg/s)")
        RAW_THROUGHPUT=$(grep -oP 'Messages Sent:\s+\d+\s+\(\K\d+\.\d+(?=\s+msg/s\))' raw_perf_results.txt | head -1 || echo "0")
        
        # Extract P95 latency (matches format: "   P95 Latency:       123.4 ms")
        P95_LATENCY=$(grep -oP 'P95 Latency:\s+\K\d+\.\d+(?=\s+ms)' latency_perf_results.txt | head -1 || echo "999")
        
        # Extract FerrisStreams SQL benchmark metrics
        echo "=== FerrisStreams SQL Benchmark Results (Throughput lines) ==="
        if [ -f ferris_sql_benchmarks.txt ]; then
          grep "records/sec" ferris_sql_benchmarks.txt || echo "No throughput metrics found in ferris_sql_benchmarks.txt"
          
          # Extract baseline throughput (format: "- Baseline Throughput: 1234 records/sec")
          BASELINE_THROUGHPUT=$(grep -oP '- Baseline Throughput:\s+\K\d+(?=\s+records/sec)' ferris_sql_benchmarks.txt | head -1 || echo "0")
          
          # Extract aggregation throughput (format: "- Aggregation Throughput: 567 records/sec")  
          AGGREGATION_THROUGHPUT=$(grep -oP '- Aggregation Throughput:\s+\K\d+(?=\s+records/sec)' ferris_sql_benchmarks.txt | head -1 || echo "0")
          
          # Extract processor comparison ratio by calculating Simple/Transactional ratio
          SIMPLE_THROUGHPUT=$(grep -oP 'Simple Processor:\s+\K\d+(?=\s+records/sec)' ferris_sql_benchmarks.txt | head -1 || echo "0")
          TRANSACTIONAL_THROUGHPUT=$(grep -oP 'Transactional Processor:\s+\K\d+(?=\s+records/sec)' ferris_sql_benchmarks.txt | head -1 || echo "1")
          if [ "$TRANSACTIONAL_THROUGHPUT" != "0" ] && [ "$SIMPLE_THROUGHPUT" != "0" ]; then
            PROCESSOR_RATIO=$(echo "scale=1; $SIMPLE_THROUGHPUT / $TRANSACTIONAL_THROUGHPUT" | bc -l || echo "1.0")
          else
            PROCESSOR_RATIO="1.0"
          fi
        else
          BASELINE_THROUGHPUT="0"
          AGGREGATION_THROUGHPUT="0" 
          PROCESSOR_RATIO="1.0"
        fi
        
        echo "JSON_THROUGHPUT=$JSON_THROUGHPUT" >> $GITHUB_ENV
        echo "RAW_THROUGHPUT=$RAW_THROUGHPUT" >> $GITHUB_ENV  
        echo "P95_LATENCY=$P95_LATENCY" >> $GITHUB_ENV
        echo "BASELINE_THROUGHPUT=$BASELINE_THROUGHPUT" >> $GITHUB_ENV
        echo "AGGREGATION_THROUGHPUT=$AGGREGATION_THROUGHPUT" >> $GITHUB_ENV
        echo "PROCESSOR_RATIO=$PROCESSOR_RATIO" >> $GITHUB_ENV
        
        echo "Performance Metrics:"
        echo "JSON Throughput: $JSON_THROUGHPUT msg/s"
        echo "Raw Throughput: $RAW_THROUGHPUT msg/s"
        echo "P95 Latency: $P95_LATENCY ms"
        echo "SQL Baseline Throughput: $BASELINE_THROUGHPUT records/sec"
        echo "SQL Aggregation Throughput: $AGGREGATION_THROUGHPUT records/sec"
        echo "Processor Performance Ratio: ${PROCESSOR_RATIO}x faster"
    
    - name: Performance Regression Check
      run: |
        echo "Checking for performance regressions..."
        
        # Define minimum acceptable performance thresholds
        MIN_JSON_THROUGHPUT=100
        MIN_RAW_THROUGHPUT=100
        MAX_P95_LATENCY=100
        
        # Check JSON throughput
        if (( $(echo "$JSON_THROUGHPUT < $MIN_JSON_THROUGHPUT" | bc -l) )); then
          echo "❌ JSON throughput regression: $JSON_THROUGHPUT < $MIN_JSON_THROUGHPUT msg/s"
          exit 1
        fi
        
        # Check Raw throughput
        if (( $(echo "$RAW_THROUGHPUT < $MIN_RAW_THROUGHPUT" | bc -l) )); then
          echo "❌ Raw throughput regression: $RAW_THROUGHPUT < $MIN_RAW_THROUGHPUT msg/s"
          exit 1
        fi
        
        # Check P95 latency
        if (( $(echo "$P95_LATENCY > $MAX_P95_LATENCY" | bc -l) )); then
          echo "❌ Latency regression: $P95_LATENCY > $MAX_P95_LATENCY ms"
          exit 1
        fi
        
        # Check SQL performance thresholds (more realistic for CI)
        MIN_SQL_BASELINE=10
        MIN_SQL_AGGREGATION=5
        MIN_PROCESSOR_RATIO=1.5
        
        # Check SQL baseline throughput (warn only if SQL benchmarks failed to run)
        if [ "$BASELINE_THROUGHPUT" = "0" ]; then
          echo "⚠️ SQL baseline benchmarks did not run - this is expected in some CI environments"
        elif (( $(echo "$BASELINE_THROUGHPUT < $MIN_SQL_BASELINE" | bc -l) )); then
          echo "❌ SQL baseline regression: $BASELINE_THROUGHPUT < $MIN_SQL_BASELINE records/sec"
          exit 1
        else
          echo "✅ SQL baseline performance: $BASELINE_THROUGHPUT records/sec"
        fi
        
        # Check SQL aggregation throughput (warn only if SQL benchmarks failed to run)
        if [ "$AGGREGATION_THROUGHPUT" = "0" ]; then
          echo "⚠️ SQL aggregation benchmarks did not run - this is expected in some CI environments"
        elif (( $(echo "$AGGREGATION_THROUGHPUT < $MIN_SQL_AGGREGATION" | bc -l) )); then
          echo "❌ SQL aggregation regression: $AGGREGATION_THROUGHPUT < $MIN_SQL_AGGREGATION records/sec"
          exit 1
        else
          echo "✅ SQL aggregation performance: $AGGREGATION_THROUGHPUT records/sec"
        fi
        
        # Check processor performance ratio (warn only if processor benchmarks failed to run)
        if [ "$PROCESSOR_RATIO" = "1.0" ] && [ "$SIMPLE_THROUGHPUT" = "0" ]; then
          echo "⚠️ Processor comparison benchmarks did not run - this is expected in some CI environments"
        elif (( $(echo "$PROCESSOR_RATIO < $MIN_PROCESSOR_RATIO" | bc -l) )); then
          echo "❌ Processor performance regression: ${PROCESSOR_RATIO}x < ${MIN_PROCESSOR_RATIO}x improvement"
          exit 1
        else
          echo "✅ Processor performance ratio: ${PROCESSOR_RATIO}x"
        fi
        
        echo "✅ All performance checks passed!"

    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: |
          json_perf_results.txt
          raw_perf_results.txt
          latency_perf_results.txt
          resource_perf_results.txt
          ferris_sql_benchmarks.txt
    
    - name: Comment Performance Results (PR only)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const jsonThroughput = process.env.JSON_THROUGHPUT;
          const rawThroughput = process.env.RAW_THROUGHPUT;
          const p95Latency = process.env.P95_LATENCY;
          const baselineThroughput = process.env.BASELINE_THROUGHPUT;
          const aggregationThroughput = process.env.AGGREGATION_THROUGHPUT;
          const processorRatio = process.env.PROCESSOR_RATIO;
          
          const comment = `## 📊 Performance Test Results
          
          ### Kafka Streaming Performance
          | Metric | Value | Status |
          |--------|-------|--------|
          | JSON Throughput | ${jsonThroughput} msg/s | ${jsonThroughput > 1000 ? '✅' : '❌'} |
          | Raw Throughput | ${rawThroughput} msg/s | ${rawThroughput > 5000 ? '✅' : '❌'} |
          | P95 Latency | ${p95Latency} ms | ${p95Latency < 100 ? '✅' : '❌'} |
          
          ### FerrisStreams SQL Performance
          | Metric | Value | Status |
          |--------|-------|--------|
          | SQL Baseline | ${baselineThroughput} records/sec | ${baselineThroughput > 50 ? '✅' : '❌'} |
          | SQL Aggregation | ${aggregationThroughput} records/sec | ${aggregationThroughput > 10 ? '✅' : '❌'} |
          | Processor Improvement | ${processorRatio}x faster | ${processorRatio > 5.0 ? '✅' : '❌'} |
          
          Performance tests completed automatically. Check the [workflow run](${context.payload.pull_request.html_url}/checks) for detailed results.`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });